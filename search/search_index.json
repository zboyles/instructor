{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Instructor (openai_function_call)","text":"<p>Renaming from openai_function_call</p> <p>This library used to be called <code>openai_function_call</code> simply change the import and you should be good to go!</p> <pre><code>find /path/to/dir -type f -exec sed -i 's/openai_function_call/instructor/g' {} \\;\n</code></pre> <p>Structured extraction in Python, powered by OpenAI's function calling api, designed for simplicity, transparency, and control.</p> <p>This library is built to interact with openai's function call api from python code, with python structs / objects. It's designed to be intuitive, easy to use, but give great visibily in how we call openai.</p> <p>The approach of combining a human prompt and a \"response schema\" is not necessarily unique; however, it shows great promise. As we have been concentrating on translating user intent into structured data, we have discovered that Python with Pydantic is exceptionally well-suited for this task. </p> <p>OpenAISchema is based on Python type annotations, and powered by Pydantic.</p> <p>The key features are:</p> <ul> <li>Intuitive to write: Great support for editors, completions. Spend less time debugging.</li> <li>Writing prompts as code: Collocate docstrings and descriptions as part of your prompting.</li> <li>Extensible: Bring your own kitchen sink without being weighted down by abstractions.</li> </ul>"},{"location":"#structured-extraction-with-openai","title":"Structured Extraction with <code>openai</code>","text":"<p>Welcome to the Quick Start Guide for OpenAI Function Call. This guide will walk you through the installation process and provide examples demonstrating the usage of function calls and schemas with OpenAI and Pydantic.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>This library depends on Pydantic and OpenAI that's all.</p>"},{"location":"#installation","title":"Installation","text":"<p>To get started with OpenAI Function Call, you need to install it using <code>pip</code>. Run the following command in your terminal:</p> <p>Note</p> <p>Ensure you have Python version 3.9 or above.</p> <pre><code>$ pip install instructor\n</code></pre>"},{"location":"#quick-start-with-patching-chatcompletion","title":"Quick Start with Patching ChatCompletion","text":"<p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the `ChatCompletion`` class. Here's a step-by-step guide:</p>"},{"location":"#step-1-import-and-patch-the-module","title":"Step 1: Import and Patch the Module","text":"<p>First, import the required libraries and apply the patch function to the OpenAI module. This exposes new functionality with the response_model parameter.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from openai.ChatCompletion.create\ninstructor.patch()\n</code></pre>"},{"location":"#step-2-define-the-pydantic-model","title":"Step 2: Define the Pydantic Model","text":"<p>Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>class UserDetail(BaseModel):\n    name: str\n    age: int\n</code></pre>"},{"location":"#step-3-extract-data-with-chatcompletion","title":"Step 3: Extract Data with ChatCompletion","text":"<p>Use the openai.ChatCompletion.create method to send a prompt and extract the data into the Pydantic object. The response_model parameter specifies the Pydantic model to use for extraction.</p> <pre><code>user: UserDetail = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n</code></pre>"},{"location":"#step-4-validate-the-extracted-data","title":"Step 4: Validate the Extracted Data","text":"<p>You can then validate the extracted data by asserting the expected values. By adding the type things you also get a bunch of nice benefits with your IDE like spell check and auto complete!</p> <pre><code>assert user.name == \"Jason\"\nassert user.age == 25\n</code></pre>"},{"location":"#introduction-to-openaischema","title":"Introduction to <code>OpenAISchema</code>","text":"<p>If you want more control than just passing a single class we can use the <code>OpenAISchema</code> which extends <code>BaseModel</code>.</p> <p>This quick start guide contains the follow sections:</p> <ol> <li>Defining a schema </li> <li>Adding Additional Prompting</li> <li>Calling the ChatCompletion</li> <li>Deserializing back to the instance</li> </ol> <p>OpenAI Function Call allows you to leverage OpenAI's powerful language models for function calls and schema extraction. This guide provides a quick start for using OpenAI Function Call.</p>"},{"location":"#section-1-defining-a-schema","title":"Section 1: Defining a Schema","text":"<p>To begin, let's define a schema using OpenAI Function Call. A schema describes the structure of the input and output data for a function. In this example, we'll define a simple schema for a <code>User</code> object:</p> <pre><code>from instructor import OpenAISchema\n\nclass UserDetails(OpenAISchema):\n    name: str\n    age: int\n</code></pre> <p>In this schema, we define a <code>UserDetails</code> class that extends <code>OpenAISchema</code>. We declare two fields, <code>name</code> and <code>age</code>, of type <code>str</code> and <code>int</code> respectively. </p>"},{"location":"#section-2-adding-additional-prompting","title":"Section 2: Adding Additional Prompting","text":"<p>To enhance the performance of the OpenAI language model, you can add additional prompting in the form of docstrings and field descriptions. They can provide context and guide the model on how to process the data.</p> <p>!!! note Using <code>patch</code>     these docstrings and fields descriptions are powered by <code>pydantic.BaseModel</code> so they'll work via the patching approach as well.</p> <pre><code>from instructor import OpenAISchema\nfrom pydantic import Field\n\nclass UserDetails(OpenAISchema):\n    \"\"\"\"\n    Correctly extracted user information\n    :param age: age of the user\n    \"\"\"\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n</code></pre> <p>In this updated schema, we use the <code>Field</code> class from <code>pydantic</code> to add descriptions to the <code>name</code> field. Moreover, we use the docstring to add information for the parameter <code>age</code>.  In both cases, the description provides information about the fields, giving even more context to the language model. Information from the docstring is extracted using docstring-parser which supports different docstring styles. Note that if the <code>Field</code> contains a description for a parameter as well as the docstring, the <code>Field</code>'s description is used. </p> <p>Code, schema, and prompt</p> <p>We can run <code>openai_schema</code> to see exactly what the API will see, notice how the docstrings, attributes, types, and parameter descriptions are now part of the schema. This describes on this library's core philosophies.</p> <pre><code>class UserDetails(OpenAISchema):\n    \"\"\"\n    Correctly extracted user information\n\n    :param name: the user's full name\n    :param age: age of the user\n    \"\"\"\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n\nUserDetails.openai_schema\n</code></pre> <pre><code>{\n\"name\": \"UserDetails\",\n\"description\": \"Correctly extracted user information\",\n\"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n    \"name\": {\n        \"type\": \"string\",\n        \"description\": \"User's full name\"\n    },\n    \"age\": {\n        \"type\": \"integer\"\n        \"description\": \"age of the user\"\n    }\n    },\n    \"required\": [\n    \"age\",\n    \"name\"\n    ]\n}\n}\n</code></pre>"},{"location":"#section-3-calling-the-chatcompletion","title":"Section 3: Calling the ChatCompletion","text":"<p>With the schema defined, let's proceed with calling the <code>ChatCompletion</code> API using the defined schema and messages.</p> <pre><code>from instructor import OpenAISchema\nfrom pydantic import Field\n\nclass UserDetails(OpenAISchema):\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo-0613\",\n    functions=[UserDetails.openai_schema],\n    function_call={\"name\": UserDetails.openai_schema[\"name\"]},\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract user details from my requests\"},\n        {\"role\": \"user\", \"content\": \"My name is John Doe and I'm 30 years old.\"},\n    ],\n)\n</code></pre> <p>In this example, we make a call to the <code>ChatCompletion</code> API by providing the model name (<code>gpt-3.5-turbo-0613</code>) and a list of messages. The messages consist of a system message and a user message. The system message sets the context by requesting user details, while the user message provides the input with the user's name and age.</p> <p>Note that we have omitted the additional parameters that can be included in the API request, such as <code>temperature</code>, <code>max_tokens</code>, and <code>n</code>. These parameters can be customized according to your requirements.</p>"},{"location":"#section-4-deserializing-back-to-the-instance","title":"Section 4: Deserializing Back to the Instance","text":"<p>To deserialize the response from the <code>ChatCompletion</code> API back into an instance of the <code>UserDetails</code> class, we can use the <code>from_response</code> method.</p> <pre><code>user = UserDetails.from_response(completion)\n\nprint(user.name)  # Output: John Doe\nprint(user.age)   # Output: 30\n</code></pre> <p>By calling <code>UserDetails.from_response</code>, we create an instance of the <code>UserDetails</code> class using the response from the API call. Subsequently, we can access the extracted user details through the <code>name</code> and <code>age</code> attributes of the <code>user</code> object.</p>"},{"location":"#ide-support","title":"IDE Support","text":"<p>Everything is designed for you to get the best developer experience possible, with the best editor support.</p> <p>Including autocompletion:</p> <p></p> <p>And even inline errors</p> <p></p>"},{"location":"#openai-schema-and-pydantic","title":"OpenAI Schema and Pydantic","text":"<p>This quick start guide provided you with a basic understanding of how to use OpenAI Function Call for schema extraction and function calls. You can now explore more advanced use cases and creative applications of this library.</p> <p>Since <code>UserDetails</code> is a <code>OpenAISchems</code> and a <code>pydantic.BaseModel</code> you can use inheritance and nesting to create more complex emails while avoiding code duplication</p> <pre><code>class UserDetails(OpenAISchema):\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n\nclass UserWithAddress(UserDetails):\n    address: str \n\nclass UserWithFriends(UserDetails):\n    best_friend: UserDetails\n    friends: List[UserDetails]\n</code></pre> <p>If you have any questions, feel free to leave an issue or reach out to the library's author on Twitter. For a more comprehensive solution with additional features, consider checking out MarvinAI.</p> <p>To see more examples of how we can create interesting models check out some examples.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under ther terms of the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#instructor.patch.patch","title":"<code>patch()</code>","text":"<p>Patch the <code>openai.ChatCompletion.create</code> and <code>openai.ChatCompletion.acreate</code> methods</p> <p>Enables the following features:</p> <ul> <li><code>response_model</code> parameter to parse the response from OpenAI's API</li> <li><code>max_retries</code> parameter to retry the function if the response is not valid</li> <li><code>validation_context</code> parameter to validate the response using the pydantic model</li> <li><code>strict</code> parameter to use strict json parsing</li> </ul>"},{"location":"api/#instructor.patch.patch--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nimport instructor\n\ninstructor.patch()\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nuser = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Jason is 20 years old\",\n        },\n    ],\n    response_model=User,\n    validation_context={...},\n    strict=True,\n)\n\nprint(user.model_dump())\n</code></pre>"},{"location":"api/#instructor.patch.patch--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n    \"role\": \"student\",\n}\n</code></pre> Source code in <code>instructor/patch.py</code> <pre><code>def patch():\n    \"\"\"\n    Patch the `openai.ChatCompletion.create` and `openai.ChatCompletion.acreate` methods\n\n    Enables the following features:\n\n    - `response_model` parameter to parse the response from OpenAI's API\n    - `max_retries` parameter to retry the function if the response is not valid\n    - `validation_context` parameter to validate the response using the pydantic model\n    - `strict` parameter to use strict json parsing\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    import instructor\n\n    instructor.patch()\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    user = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Jason is 20 years old\",\n            },\n        ],\n        response_model=User,\n        validation_context={...},\n        strict=True,\n    )\n\n    print(user.model_dump())\n    ```\n\n    ## Result\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n        \"role\": \"student\",\n    }\n    ```\n\n\n    \"\"\"\n    openai.ChatCompletion.create = wrap_chatcompletion(original_chatcompletion)\n    openai.ChatCompletion.acreate = wrap_chatcompletion(original_chatcompletion_async)\n</code></pre>"},{"location":"api/#instructor.patch.process_response","title":"<code>process_response(response, response_model, validation_context=None, strict=None)</code>","text":"<p>Processes a OpenAI response with the response model, if available It can use <code>validation_context</code> and <code>strict</code> to validate the response via the pydantic model</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>response_model</code> <code>BaseModel</code> <p>The response model to use for parsing the response</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response. Defaults to None.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing. Defaults to None.</p> <code>None</code> Source code in <code>instructor/patch.py</code> <pre><code>def process_response(\n    response, response_model, validation_context: dict = None, strict=None\n):  # type: ignore\n    \"\"\"Processes a OpenAI response with the response model, if available\n    It can use `validation_context` and `strict` to validate the response\n    via the pydantic model\n\n    Args:\n        response (ChatCompletion): The response from OpenAI's API\n        response_model (BaseModel): The response model to use for parsing the response\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (bool, optional): Whether to use strict json parsing. Defaults to None.\n    \"\"\"\n    if response_model is not None:\n        model = response_model.from_response(\n            response, validation_context=validation_context, strict=strict\n        )\n        model._raw_response = response\n        return model\n    return response\n</code></pre>"},{"location":"api/#instructor.dsl.validators.Validator","title":"<code>Validator</code>","text":"<p>             Bases: <code>OpenAISchema</code></p> <p>Validate if an attribute is correct and if not, return a new value with an error message</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>class Validator(instructor.OpenAISchema):\n    \"\"\"\n    Validate if an attribute is correct and if not,\n    return a new value with an error message\n    \"\"\"\n\n    is_valid: bool = Field(\n        default=True,\n        description=\"Whether the attribute is valid based on the requirements\",\n    )\n    reason: Optional[str] = Field(\n        default=None,\n        description=\"The error message if the attribute is not valid, otherwise None\",\n    )\n    fixed_value: Optional[str] = Field(\n        default=None,\n        description=\"If the attribuet is not valid, suggest a new value for the attribute\",\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.llm_validator","title":"<code>llm_validator(statement, allow_override=False, model='gpt-3.5-turbo', temperature=0)</code>","text":"<p>Create a validator that uses the LLM to validate an attribute</p>"},{"location":"api/#instructor.dsl.validators.llm_validator--usage","title":"Usage","text":"<pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")]\n    age: int = Field(description=\"The age of the person\")\n\ntry:\n    user = User(name=\"Jason Liu\", age=20)\nexcept ValidationError as e:\n    print(e)\n</code></pre> <pre><code>1 validation error for User\nname\n  The name is valid but not all lowercase (type=value_error.llm_validator)\n</code></pre> <p>Note that there, the error message is written by the LLM, and the error type is <code>value_error.llm_validator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The statement to validate</p> required <code>model</code> <code>str</code> <p>The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")</p> <code>'gpt-3.5-turbo'</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM (default: 0)</p> <code>0</code> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def llm_validator(\n    statement: str,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n):\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    ## Usage\n\n    ```python\n    from instructor import llm_validator\n    from pydantic import BaseModel, Field, field_validator\n\n    class User(BaseModel):\n        name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")]\n        age: int = Field(description=\"The age of the person\")\n\n    try:\n        user = User(name=\"Jason Liu\", age=20)\n    except ValidationError as e:\n        print(e)\n    ```\n\n    ```\n    1 validation error for User\n    name\n      The name is valid but not all lowercase (type=value_error.llm_validator)\n    ```\n\n    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n    \"\"\"\n\n    def llm(v):\n        resp = openai.ChatCompletion.create(\n            functions=[Validator.openai_schema],\n            function_call={\"name\": Validator.openai_schema[\"name\"]},\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )  # type: ignore\n        resp = Validator.from_response(resp)\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin","title":"<code>CitationMixin</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Helpful mixing that can use <code>validation_context={\"context\": context}</code> in <code>from_response</code> to find the span of the substring_phrase in the context.</p>"},{"location":"api/#instructor.dsl.citation.CitationMixin--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import CitationMixin\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\n\ncontext = \"Betty was a student. Jason was a student. Jason is 20 years old\"\n\nuser = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo',\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract jason from {context}\",\n        },\n    response_model=User,\n    validation_context={\"context\": context},\n    ]\n)\n\nfor quote in user.substring_quotes:\n    assert quote in context\n\nprint(user.model_dump())\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n    \"role\": \"student\",\n    \"substring_quotes\": [\n        \"Jason was a student\",\n        \"Jason is 20 years old\",\n    ]\n}\n</code></pre> Source code in <code>instructor/dsl/citation.py</code> <pre><code>class CitationMixin(BaseModel):\n    \"\"\"\n    Helpful mixing that can use `validation_context={\"context\": context}` in `from_response` to find the span of the substring_phrase in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import CitationMixin\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n\n    context = \"Betty was a student. Jason was a student. Jason is 20 years old\"\n\n    user = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo',\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract jason from {context}\",\n            },\n        response_model=User,\n        validation_context={\"context\": context},\n        ]\n    )\n\n    for quote in user.substring_quotes:\n        assert quote in context\n\n    print(user.model_dump())\n    ```\n\n    ## Result\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n        \"role\": \"student\",\n        \"substring_quotes\": [\n            \"Jason was a student\",\n            \"Jason is 20 years old\",\n        ]\n    }\n    ```\n\n    \"\"\"\n\n    substring_quotes: List[str] = Field(\n        description=\"List of unique and specific substrings of the quote that was used to answer the question.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"CitationMixin\":\n        \"\"\"\n        For each substring_phrase, find the span of the substring_phrase in the context.\n        If the span is not found, remove the substring_phrase from the list.\n        \"\"\"\n        if info.context is None:\n            return self\n\n        # Get the context from the info\n        text_chunks = info.context.get(\"context\", None)\n\n        # Get the spans of the substring_phrase in the context\n        spans = list(self.get_spans(text_chunks))\n        # Replace the substring_phrase with the actual substring\n        self.substring_quotes = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def _get_span(self, quote, context, errs=5):\n        import regex\n\n        minor = quote\n        major = context\n\n        errs_ = 0\n        s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n        while s is None and errs_ &lt;= errs:\n            errs_ += 1\n            s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n\n        if s is not None:\n            yield from s.spans()\n\n    def get_spans(self, context):\n        for quote in self.substring_quotes:\n            yield from self._get_span(quote, context)\n</code></pre>"},{"location":"api/#instructor.dsl.citation.CitationMixin.validate_sources","title":"<code>validate_sources(info)</code>","text":"<p>For each substring_phrase, find the span of the substring_phrase in the context. If the span is not found, remove the substring_phrase from the list.</p> Source code in <code>instructor/dsl/citation.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_sources(self, info: FieldValidationInfo) -&gt; \"CitationMixin\":\n    \"\"\"\n    For each substring_phrase, find the span of the substring_phrase in the context.\n    If the span is not found, remove the substring_phrase from the list.\n    \"\"\"\n    if info.context is None:\n        return self\n\n    # Get the context from the info\n    text_chunks = info.context.get(\"context\", None)\n\n    # Get the spans of the substring_phrase in the context\n    spans = list(self.get_spans(text_chunks))\n    # Replace the substring_phrase with the actual substring\n    self.substring_quotes = [text_chunks[span[0] : span[1]] for span in spans]\n    return self\n</code></pre>"},{"location":"api/#instructor.dsl.multitask.MultiTask","title":"<code>MultiTask(subtask_class, name=None, description=None)</code>","text":"<p>Dynamically create a MultiTask OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden.</p>"},{"location":"api/#instructor.dsl.multitask.MultiTask--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import MultiTask\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMultiUser = MultiTask(User)\n</code></pre>"},{"location":"api/#instructor.dsl.multitask.MultiTask--result","title":"Result","text":"<pre><code>class MultiUser(OpenAISchema, MultiTaskBase):\n    tasks: List[User] = Field(\n        default_factory=list,\n        repr=False,\n        description=\"Correctly segmented list of `User` tasks\",\n    )\n\n    @classmethod\n    def from_streaming_response(cls, completion) -&gt; Generator[User]:\n        '''\n        Parse the streaming response from OpenAI and yield a `User` object\n        for each task in the response\n        '''\n        json_chunks = cls.extract_json(completion)\n        yield from cls.tasks_from_chunks(json_chunks)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subtask_class</code> <code>Type[OpenAISchema]</code> <p>The base class to use for the MultiTask</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the MultiTask class, if None then the name of the subtask class is used as <code>Multi{subtask_class.__name__}</code></p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the MultiTask class, if None then the description is set to <code>Correct segmentation of</code>{subtask_class.name}<code>tasks</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>schema</code> <code>OpenAISchema</code> <p>A new class that can be used to segment multiple tasks</p> Source code in <code>instructor/dsl/multitask.py</code> <pre><code>def MultiTask(\n    subtask_class: Type[BaseModel],\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n):\n    \"\"\"\n    Dynamically create a MultiTask OpenAISchema that can be used to segment multiple\n    tasks given a base class. This creates class that can be used to create a toolkit\n    for a specific task, names and descriptions are automatically generated. However\n    they can be overridden.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import MultiTask\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MultiUser = MultiTask(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MultiUser(OpenAISchema, MultiTaskBase):\n        tasks: List[User] = Field(\n            default_factory=list,\n            repr=False,\n            description=\"Correctly segmented list of `User` tasks\",\n        )\n\n        @classmethod\n        def from_streaming_response(cls, completion) -&gt; Generator[User]:\n            '''\n            Parse the streaming response from OpenAI and yield a `User` object\n            for each task in the response\n            '''\n            json_chunks = cls.extract_json(completion)\n            yield from cls.tasks_from_chunks(json_chunks)\n    ```\n\n    Parameters:\n        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask\n        name (Optional[str]): The name of the MultiTask class, if None then the name\n            of the subtask class is used as `Multi{subtask_class.__name__}`\n        description (Optional[str]): The description of the MultiTask class, if None\n            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`\n\n    Returns:\n        schema (OpenAISchema): A new class that can be used to segment multiple tasks\n    \"\"\"\n    task_name = subtask_class.__name__ if name is None else name\n\n    name = f\"Multi{task_name}\"\n\n    list_tasks = (\n        List[subtask_class],\n        Field(\n            default_factory=list,\n            repr=False,\n            description=f\"Correctly segmented list of `{task_name}` tasks\",\n        ),\n    )\n\n    new_cls = create_model(\n        name,\n        tasks=list_tasks,\n        __base__=(OpenAISchema, MultiTaskBase),  # type: ignore\n    )\n    # set the class constructor BaseModel\n    new_cls.task_type = subtask_class\n\n    new_cls.__doc__ = (\n        f\"Correct segmentation of `{task_name}` tasks\"\n        if description is None\n        else description\n    )\n\n    return new_cls\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.MaybeBase","title":"<code>MaybeBase</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Extract a result from a model, if any, otherwise set the error and message fields.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>class MaybeBase(BaseModel):\n    \"\"\"\n    Extract a result from a model, if any, otherwise set the error and message fields.\n    \"\"\"\n\n    result: Optional[BaseModel]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None  # type: ignore\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe","title":"<code>Maybe(model)</code>","text":"<p>Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code> for sitatations where the data may not be present in the context.</p>"},{"location":"api/#instructor.dsl.maybe.Maybe--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import Maybe\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMaybeUser = Maybe(User)\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe--result","title":"Result","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[User]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to wrap with Maybe.</p> required <p>Returns:</p> Name Type Description <code>MaybeModel</code> <code>Type[BaseModel]</code> <p>A new Pydantic model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code>.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>def Maybe(model: Type[BaseModel]) -&gt; MaybeBase:\n    \"\"\"\n    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import Maybe\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MaybeUser = Maybe(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MaybeUser(BaseModel):\n        result: Optional[User]\n        error: bool = Field(default=False)\n        message: Optional[str]\n\n        def __bool__(self):\n            return self.result is not None\n    ```\n\n    Parameters:\n        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.\n\n    Returns:\n        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.\n    \"\"\"\n\n    class MaybeBase(BaseModel):\n        def __bool__(self):\n            return self.result is not None  # type: ignore\n\n    fields = {\n        \"result\": (\n            Optional[model],\n            Field(\n                default=None,\n                description=\"Correctly extracted result from the model, if any, otherwise None\",\n            ),\n        ),\n        \"error\": (bool, Field(default=False)),\n        \"message\": (\n            Optional[str],\n            Field(\n                default=None,\n                description=\"Error message if no result was found, should be short and concise\",\n            ),\n        ),\n    }\n\n    MaybeModel = create_model(f\"Maybe{model.__name__}\", __base__=MaybeBase, **fields)\n\n    return MaybeModel  # type: ignore\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Augments a Pydantic model with OpenAI's schema for function calling</p> <p>This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.</p>"},{"location":"api/#instructor.function_calls.OpenAISchema--usage","title":"Usage","text":"<pre><code>from instructor import OpenAISchema\n\nclass User(OpenAISchema):\n    name: str\n    age: int\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo',\n    messages=[{\n        \"content\": \"Jason is 20 years old\",\n        \"role\": \"user\"\n    }],\n    functions=[User.openai_schema],\n    function_call={\"name\": User.openai_schema[\"name\"]},\n)\n\nuser = User.from_response(completion)\n\nprint(user.model_dump())\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n}\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):\n    \"\"\"\n    Augments a Pydantic model with OpenAI's schema for function calling\n\n    This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.\n\n    ## Usage\n\n    ```python\n    from instructor import OpenAISchema\n\n    class User(OpenAISchema):\n        name: str\n        age: int\n\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo',\n        messages=[{\n            \"content\": \"Jason is 20 years old\",\n            \"role\": \"user\"\n        }],\n        functions=[User.openai_schema],\n        function_call={\"name\": User.openai_schema[\"name\"]},\n    )\n\n    user = User.from_response(completion)\n\n    print(user.model_dump())\n    ```\n    ## Result\n\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n    }\n    ```\n\n\n    \"\"\"\n\n    @classmethod\n    @property\n    def openai_schema(cls):\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classmethod\n    def from_response(\n        cls,\n        completion,\n        throw_error: bool = True,\n        validation_context=None,\n        strict: bool = None,\n    ):\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        message = completion[\"choices\"][0][\"message\"]\n\n        if throw_error:\n            assert \"function_call\" in message, \"No function call detected\"\n            assert (\n                message[\"function_call\"][\"name\"] == cls.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n\n        return cls.model_validate_json(\n            message[\"function_call\"][\"arguments\"],\n            context=validation_context,\n            strict=strict,\n        )\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema</code>  <code>classmethod</code> <code>property</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p>"},{"location":"api/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, throw_error=True, validation_context=None, strict=None)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> <code>True</code> <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion,\n    throw_error: bool = True,\n    validation_context=None,\n    strict: bool = None,\n):\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    message = completion[\"choices\"][0][\"message\"]\n\n    if throw_error:\n        assert \"function_call\" in message, \"No function call detected\"\n        assert (\n            message[\"function_call\"][\"name\"] == cls.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n\n    return cls.model_validate_json(\n        message[\"function_call\"][\"arguments\"],\n        context=validation_context,\n        strict=strict,\n    )\n</code></pre>"},{"location":"api/#instructor.function_calls.openai_function","title":"<code>openai_function</code>","text":"<p>Decorator to convert a function into an OpenAI function.</p> <p>This decorator will convert a function into an OpenAI function. The function will be validated using pydantic and the schema will be generated from the function signature.</p> Example <pre><code>@openai_function\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n\ncompletion = openai.ChatCompletion.create(\n    ...\n    messages=[{\n        \"content\": \"What is 1 + 1?\",\n        \"role\": \"user\"\n    }]\n)\nsum.from_response(completion)\n# 2\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class openai_function:\n    \"\"\"\n    Decorator to convert a function into an OpenAI function.\n\n    This decorator will convert a function into an OpenAI function. The\n    function will be validated using pydantic and the schema will be\n    generated from the function signature.\n\n    Example:\n        ```python\n        @openai_function\n        def sum(a: int, b: int) -&gt; int:\n            return a + b\n\n        completion = openai.ChatCompletion.create(\n            ...\n            messages=[{\n                \"content\": \"What is 1 + 1?\",\n                \"role\": \"user\"\n            }]\n        )\n        sum.from_response(completion)\n        # 2\n        ```\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self.func = func\n        self.validate_func = validate_arguments(func)\n        self.docstring = parse(self.func.__doc__ or \"\")\n\n        parameters = self.validate_func.model.model_json_schema()\n        parameters[\"properties\"] = {\n            k: v\n            for k, v in parameters[\"properties\"].items()\n            if k not in (\"v__duplicate_kwargs\", \"args\", \"kwargs\")\n        }\n        for param in self.docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                parameters[\"properties\"][name][\"description\"] = description\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n        self.openai_schema = {\n            \"name\": self.func.__name__,\n            \"description\": self.docstring.short_description,\n            \"parameters\": parameters,\n        }\n        self.model = self.validate_func.model\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        @wraps(self.func)\n        def wrapper(*args, **kwargs):\n            return self.validate_func(*args, **kwargs)\n\n        return wrapper(*args, **kwargs)\n\n    def from_response(self, completion, throw_error=True, strict: bool = None):\n        \"\"\"\n        Parse the response from OpenAI's API and return the function call\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from OpenAI's API\n            throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n        Returns:\n            result (any): result of the function call\n        \"\"\"\n        message = completion[\"choices\"][0][\"message\"]\n\n        if throw_error:\n            assert \"function_call\" in message, \"No function call detected\"\n            assert (\n                message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n\n        function_call = message[\"function_call\"]\n        arguments = json.loads(function_call[\"arguments\"], strict=strict)\n        return self.validate_func(**arguments)\n</code></pre>"},{"location":"api/#instructor.function_calls.openai_function.from_response","title":"<code>from_response(completion, throw_error=True, strict=None)</code>","text":"<p>Parse the response from OpenAI's API and return the function call</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the response does not contain a function call</p> <code>True</code> <p>Returns:</p> Name Type Description <code>result</code> <code>any</code> <p>result of the function call</p> Source code in <code>instructor/function_calls.py</code> <pre><code>def from_response(self, completion, throw_error=True, strict: bool = None):\n    \"\"\"\n    Parse the response from OpenAI's API and return the function call\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from OpenAI's API\n        throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n    Returns:\n        result (any): result of the function call\n    \"\"\"\n    message = completion[\"choices\"][0][\"message\"]\n\n    if throw_error:\n        assert \"function_call\" in message, \"No function call detected\"\n        assert (\n            message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n\n    function_call = message[\"function_call\"]\n    arguments = json.loads(function_call[\"arguments\"], strict=strict)\n    return self.validate_func(**arguments)\n</code></pre>"},{"location":"chat-completion/","title":"Using the Prompt Pipeline","text":"<p>To use the Prompt Pipeline in OpenAI Function Call, you need to instantiate a <code>ChatCompletion</code> object and build the API call by piping messages and functions to it.</p>"},{"location":"chat-completion/#the-chatcompletion-object","title":"The ChatCompletion Object","text":"<p>The <code>ChatCompletion</code> object is the starting point for constructing your API call. It provides the necessary methods and attributes to define the conversation flow and include function calls.</p>"},{"location":"chat-completion/#instructor.dsl.completion.ChatCompletion","title":"<code>ChatCompletion</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A chat completion is a collection of messages and configration options that can be used to generate a chat response from the OpenAI API.</p> Usage <p>In order to generate a chat response from the OpenAI API, you need to create a chat completion and then pipe it to a message and a <code>OpenAISchema</code>. Then when <code>create</code> or <code>acreate</code> is called we'll return the response from the API as an instance of <code>OpenAISchema</code>.</p> Example <pre><code>class Sum(OpenAISchema):\n    a: int\n    b: int\n\ncompletion = (\n    ChatCompletion(\"example\")\n    | TaggedMessage(content=\"What is 1 + 1?\", tag=\"question\")\n    | Schema\n)\n\nprint(completion.create())\n# Sum(a=1, b=1)\n</code></pre> Tips <ul> <li>You can use the <code>|</code> operator to chain multiple messages and functions together</li> <li>There should be exactly one function call class (OpenAISchema) per chat completion</li> <li>System messages will be concatenated together</li> <li>Only one chain of thought message can be used per completion</li> </ul> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the chat completion</p> <code>model</code> <code>str</code> <p>The model to use for the chat completion (default: \"gpt-3.5-turbo-0613\")</p> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate (default: 1000)</p> <code>temperature</code> <code>float</code> <p>The temperature to use for the chat completion (default: 0.1)</p> <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API (default: False)</p> Warning <p>Currently we do not support streaming the response from the API, so the stream parameter is not supported yet.</p> Source code in <code>instructor/dsl/completion.py</code> <pre><code>class ChatCompletion(BaseModel):\n    \"\"\"\n    A chat completion is a collection of messages and configration options that can be used to\n    generate a chat response from the OpenAI API.\n\n\n    Usage:\n        In order to generate a chat response from the OpenAI API, you need to create a chat completion and then pipe it to a message and a `OpenAISchema`. Then when `create` or `acreate` is called we'll return the response from the API as an instance of `OpenAISchema`.\n\n\n    Example:\n        ```python\n        class Sum(OpenAISchema):\n            a: int\n            b: int\n\n        completion = (\n            ChatCompletion(\"example\")\n            | TaggedMessage(content=\"What is 1 + 1?\", tag=\"question\")\n            | Schema\n        )\n\n        print(completion.create())\n        # Sum(a=1, b=1)\n        ```\n\n\n    Tips:\n        * You can use the `|` operator to chain multiple messages and functions together\n        * There should be exactly one function call class (OpenAISchema) per chat completion\n        * System messages will be concatenated together\n        * Only one chain of thought message can be used per completion\n\n\n    Attributes:\n        name (str): The name of the chat completion\n        model (str): The model to use for the chat completion (default: \"gpt-3.5-turbo-0613\")\n        max_tokens (int): The maximum number of tokens to generate (default: 1000)\n        temperature (float): The temperature to use for the chat completion (default: 0.1)\n        stream (bool): Whether to stream the response from the API (default: False)\n\n    Warning:\n        Currently we do not support streaming the response from the API, so the stream parameter is not supported yet.\n    \"\"\"\n\n    name: str\n    model: str = Field(default=\"gpt-3.5-turbo-0613\")\n    max_tokens: int = Field(default=1000)\n    temperature: float = Field(default=0.1)\n    stream: bool = Field(default=False)\n\n    messages: List[Message] = Field(default_factory=list, repr=False)\n    system_message: Message = Field(default=None, repr=False)\n    cot_message: ChainOfThought = Field(default=None, repr=False)\n    function: OpenAISchema = Field(default=None, repr=False)\n\n    def __post_init__(self):\n        assert not self.stream, \"Stream is not supported yet\"\n\n    def __or__(self, other: Union[Message, OpenAISchema]) -&gt; \"ChatCompletion\":\n        \"\"\"\n        Add a message or function to the chat completion, this can be used to chain multiple messages and functions together. It should contain some set of user or system messages along with a function call class (OpenAISchema)\n\n        \"\"\"\n\n        if isinstance(other, Message):\n            if other.role == MessageRole.SYSTEM:\n                if not self.system_message:\n                    self.system_message = other  # type: ignore\n                else:\n                    self.system_message.content += \"\\n\\n\" + other.content\n            else:\n                if isinstance(other, ChainOfThought):\n                    if self.cot_message:\n                        raise ValueError(\n                            \"Only one chain of thought message can be used per completion\"\n                        )\n                    self.cot_message = other\n                self.messages.append(other)\n        else:\n            if self.function:\n                raise ValueError(\n                    \"Only one function can be used per completion, wrap your tools into a single toolkit schema\"\n                )\n            self.function = other\n\n            assert self.model not in {\n                \"gpt-3.5-turbo\",\n                \"gpt-4\",\n            }, \"Only *-0613 models can currently use functions\"\n        return self\n\n    @property\n    def kwargs(self) -&gt; dict:\n        \"\"\"\n        Construct the kwargs for the OpenAI API call\n\n        Example:\n            ```python\n            result = openai.ChatCompletion.create(**self.kwargs)\n            ```\n        \"\"\"\n        kwargs = {}\n\n        messages = []\n\n        if self.system_message:\n            messages.append(self.system_message.dict())\n\n        if self.messages:\n            special_types = {\n                SystemMessage,\n                ChainOfThought,\n            }\n            messages += [\n                message.dict()\n                for message in self.messages\n                if type(message) not in special_types\n            ]\n\n        if self.cot_message:\n            messages.append(self.cot_message.dict())\n\n        kwargs[\"messages\"] = messages\n\n        if self.function:\n            kwargs[\"functions\"] = [self.function.openai_schema]\n            kwargs[\"function_call\"] = {\"name\": self.function.openai_schema[\"name\"]}\n\n        kwargs[\"max_tokens\"] = self.max_tokens\n        kwargs[\"temperature\"] = self.temperature\n        kwargs[\"model\"] = self.model\n        return kwargs\n\n    def create(self):\n        \"\"\"\n        Create a chat response from the OpenAI API\n\n        Returns:\n            response (OpenAISchema): The response from the OpenAI API\n        \"\"\"\n        kwargs = self.kwargs\n        completion = openai.ChatCompletion.create(**kwargs)\n        if self.function:\n            return self.function.from_response(completion)\n        return completion\n\n    async def acreate(self):\n        \"\"\"\n        Create a chat response from the OpenAI API asynchronously\n\n        Returns:\n            response (OpenAISchema): The response from the OpenAI API\n        \"\"\"\n        kwargs = self.kwargs\n        completion = openai.ChatCompletion.acreate(**kwargs)\n        if self.function:\n            return self.function.from_response(await completion)\n        return await completion\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.completion.ChatCompletion.kwargs","title":"<code>kwargs: dict</code>  <code>property</code>","text":"<p>Construct the kwargs for the OpenAI API call</p> Example <pre><code>result = openai.ChatCompletion.create(**self.kwargs)\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.completion.ChatCompletion.__or__","title":"<code>__or__(other)</code>","text":"<p>Add a message or function to the chat completion, this can be used to chain multiple messages and functions together. It should contain some set of user or system messages along with a function call class (OpenAISchema)</p> Source code in <code>instructor/dsl/completion.py</code> <pre><code>def __or__(self, other: Union[Message, OpenAISchema]) -&gt; \"ChatCompletion\":\n    \"\"\"\n    Add a message or function to the chat completion, this can be used to chain multiple messages and functions together. It should contain some set of user or system messages along with a function call class (OpenAISchema)\n\n    \"\"\"\n\n    if isinstance(other, Message):\n        if other.role == MessageRole.SYSTEM:\n            if not self.system_message:\n                self.system_message = other  # type: ignore\n            else:\n                self.system_message.content += \"\\n\\n\" + other.content\n        else:\n            if isinstance(other, ChainOfThought):\n                if self.cot_message:\n                    raise ValueError(\n                        \"Only one chain of thought message can be used per completion\"\n                    )\n                self.cot_message = other\n            self.messages.append(other)\n    else:\n        if self.function:\n            raise ValueError(\n                \"Only one function can be used per completion, wrap your tools into a single toolkit schema\"\n            )\n        self.function = other\n\n        assert self.model not in {\n            \"gpt-3.5-turbo\",\n            \"gpt-4\",\n        }, \"Only *-0613 models can currently use functions\"\n    return self\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.completion.ChatCompletion.acreate","title":"<code>acreate()</code>  <code>async</code>","text":"<p>Create a chat response from the OpenAI API asynchronously</p> <p>Returns:</p> Name Type Description <code>response</code> <code>OpenAISchema</code> <p>The response from the OpenAI API</p> Source code in <code>instructor/dsl/completion.py</code> <pre><code>async def acreate(self):\n    \"\"\"\n    Create a chat response from the OpenAI API asynchronously\n\n    Returns:\n        response (OpenAISchema): The response from the OpenAI API\n    \"\"\"\n    kwargs = self.kwargs\n    completion = openai.ChatCompletion.acreate(**kwargs)\n    if self.function:\n        return self.function.from_response(await completion)\n    return await completion\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.completion.ChatCompletion.create","title":"<code>create()</code>","text":"<p>Create a chat response from the OpenAI API</p> <p>Returns:</p> Name Type Description <code>response</code> <code>OpenAISchema</code> <p>The response from the OpenAI API</p> Source code in <code>instructor/dsl/completion.py</code> <pre><code>def create(self):\n    \"\"\"\n    Create a chat response from the OpenAI API\n\n    Returns:\n        response (OpenAISchema): The response from the OpenAI API\n    \"\"\"\n    kwargs = self.kwargs\n    completion = openai.ChatCompletion.create(**kwargs)\n    if self.function:\n        return self.function.from_response(completion)\n    return completion\n</code></pre>"},{"location":"chat-completion/#messages-types","title":"Messages Types","text":"<p>The basis of a message is defined as a <code>dataclass</code>. However, we provide helper functions and classes that provide additional functionality in the form of templates. </p>"},{"location":"chat-completion/#instructor.dsl.messages.base.Message","title":"<code>Message</code>","text":"<p>A message class that helps build messages for the chat interface.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The content of the message.</p> <code>role</code> <code>MessageRole</code> <p>The role of the message.</p> <code>name</code> <code>Optional[str]</code> <p>The name of the user, only used if the role is USER.</p> Tips <p>If you want to make custom messages simple make a function that returns the <code>Message</code> class and use that as part of your pipes. For example if you want to add additional context:</p> <pre><code>def GetUserData(user_id) -&gt; Message:\n    data = ...\n    return Message(\n        content=\"This is some more user data: {data} for {user_id}\n        role=MessageRole.USER\n    )\n</code></pre> Source code in <code>instructor/dsl/messages/base.py</code> <pre><code>@dataclass\nclass Message:\n    \"\"\"\n    A message class that helps build messages for the chat interface.\n\n    Attributes:\n        content (str): The content of the message.\n        role (MessageRole): The role of the message.\n        name (Optional[str]): The name of the user, only used if the role is USER.\n\n    Tips:\n        If you want to make custom messages simple make a function that returns the `Message` class and use that as part of your pipes. For example if you want to add additional context:\n\n        ```python\n        def GetUserData(user_id) -&gt; Message:\n            data = ...\n            return Message(\n                content=\"This is some more user data: {data} for {user_id}\n                role=MessageRole.USER\n            )\n        ```\n    \"\"\"\n\n    content: str = Field(default=None, repr=True)\n    role: MessageRole = Field(default=MessageRole.USER, repr=False)\n    name: Optional[str] = Field(default=None)\n\n    def dict(self):\n        assert self.content is not None, \"Content must be set!\"\n        obj = {\n            \"role\": self.role.name.lower(),\n            \"content\": self.content,\n        }\n        if self.name and self.role == MessageRole.USER:\n            obj[\"name\"] = self.name\n        return obj\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.base.MessageRole","title":"<code>MessageRole</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enum that represents the role of a message.</p> <p>Attributes:</p> Name Type Description <code>USER</code> <p>A message from the user.</p> <code>SYSTEM</code> <p>A message from the system.</p> <code>ASSISTANT</code> <p>A message from the assistant.</p> Source code in <code>instructor/dsl/messages/base.py</code> <pre><code>class MessageRole(Enum):\n    \"\"\"\n    An enum that represents the role of a message.\n\n    Attributes:\n        USER: A message from the user.\n        SYSTEM: A message from the system.\n        ASSISTANT: A message from the assistant.\n    \"\"\"\n\n    USER = auto()\n    SYSTEM = auto()\n    ASSISTANT = auto()\n</code></pre>"},{"location":"chat-completion/#helper-messages-templates","title":"Helper Messages / Templates","text":""},{"location":"chat-completion/#instructor.dsl.messages.messages.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>             Bases: <code>Message</code></p> <p>Special message type to correctly leverage chain of thought reasoning for the task. This is automatically set as the last message.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>@dataclass\nclass ChainOfThought(Message):\n    \"\"\"\n    Special message type to correctly leverage chain of thought reasoning\n    for the task. This is automatically set as the last message.\n    \"\"\"\n\n    def __post_init__(self):\n        self.content = \"Lets think step by step to get the correct answer:\"\n        self.role = MessageRole.ASSISTANT\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemGuidelines","title":"<code>SystemGuidelines(guidelines)</code>","text":"<p>Create a system message that tells the user what guidelines they must follow when responding.</p> <p>Parameters:</p> Name Type Description Default <code>guidelines</code> <code>List[str]</code> <p>The guidelines the user must follow when responding.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message that tells the user what guidelines they must follow when responding.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemGuidelines(guidelines: List[str]) -&gt; Message:\n    \"\"\"\n    Create a system message that tells the user what guidelines they must follow when responding.\n\n    Parameters:\n        guidelines (List[str]): The guidelines the user must follow when responding.\n\n    Returns:\n        message (Message): A system message that tells the user what guidelines they must follow when responding.\n    \"\"\"\n    guideline_str = \"\\n* \".join(guidelines)\n    return Message(\n        content=f\"Here are the guidelines you must to follow when responding:\\n\\n* {guideline_str}\",\n        role=MessageRole.SYSTEM,\n    )\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemIdentity","title":"<code>SystemIdentity(identity)</code>","text":"<p>Create a system message that tells the user what their identity is.</p> <p>Parameters:</p> Name Type Description Default <code>identity</code> <code>str</code> <p>The identity of the user.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message that tells the user what their identity is.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemIdentity(identity: str) -&gt; Message:\n    \"\"\"\n    Create a system message that tells the user what their identity is.\n\n    Parameters:\n        identity (str): The identity of the user.\n\n    Returns:\n        message (Message): A system message that tells the user what their identity is.\n    \"\"\"\n    return Message(content=f\"You are a {identity.lower()}.\", role=MessageRole.SYSTEM)\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemMessage","title":"<code>SystemMessage(content)</code>","text":"<p>Create a system message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemMessage(content: str) -&gt; Message:\n    \"\"\"\n    Create a system message.\n\n    Parameters:\n        content (str): The content of the message.\n\n    Returns:\n        message (Message): A system message.\"\"\"\n    return Message(content=content, role=MessageRole.SYSTEM)\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemStyle","title":"<code>SystemStyle(style)</code>","text":"<p>Create a system message that tells the user what style they are responding in.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The style the user is responding in.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message that tells the user what style they are responding in.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemStyle(style: str) -&gt; Message:\n    \"\"\"\n    Create a system message that tells the user what style they are responding in.\n\n    Parameters:\n        style (str): The style the user is responding in.\n\n    Returns:\n        message (Message): A system message that tells the user what style they are responding in.\n    \"\"\"\n    return Message(\n        content=f\"You must respond with in following style: {style.lower()}.\",\n        role=MessageRole.SYSTEM,\n    )\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemTask","title":"<code>SystemTask(task)</code>","text":"<p>Create a system message that tells the user what task they are doing, uses language to push the system to behave as a world class algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The task the user is doing.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message that tells the user what task they are doing.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemTask(task: str) -&gt; Message:\n    \"\"\"\n    Create a system message that tells the user what task they are doing, uses language to\n    push the system to behave as a world class algorithm.\n\n    Parameters:\n        task (str): The task the user is doing.\n\n    Returns:\n        message (Message): A system message that tells the user what task they are doing.\n    \"\"\"\n    return Message(\n        content=f\"You are a world class state of the art algorithm capable of correctly completing the following task: `{task}`.\",\n        role=MessageRole.SYSTEM,\n    )\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.messages.SystemTips","title":"<code>SystemTips(tips)</code>","text":"<p>Create a system message that gives the user some tips before responding.</p> <p>Parameters:</p> Name Type Description Default <code>tips</code> <code>List[str]</code> <p>The tips the user should follow when responding.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A system message that gives the user some tips before responding.</p> Source code in <code>instructor/dsl/messages/messages.py</code> <pre><code>def SystemTips(tips: List[str]) -&gt; Message:\n    \"\"\"\n    Create a system message that gives the user some tips before responding.\n\n    Parameters:\n        tips (List[str]): The tips the user should follow when responding.\n\n    Returns:\n        message (Message): A system message that gives the user some tips before responding.\n    \"\"\"\n    tips_str = \"\\n* \".join(tips)\n    return Message(\n        content=f\"Here are some tips before responding:\\n\\n* {tips_str}\",\n        role=MessageRole.SYSTEM,\n    )\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.user.TaggedMessage","title":"<code>TaggedMessage(content, tag='data', header='Consider the following data:')</code>","text":"<p>Create a user message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <code>tag</code> <code>str</code> <p>The tag to use, will show up as content.</p> <code>'data'</code> <code>header</code> <code>str</code> <p>The header to reference the data</p> <code>'Consider the following data:'</code> <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A user message with the data tagged.</p> Source code in <code>instructor/dsl/messages/user.py</code> <pre><code>def TaggedMessage(\n    content: str, tag: str = \"data\", header: str = \"Consider the following data:\"\n) -&gt; Message:\n    \"\"\"\n    Create a user message.\n\n    Parameters:\n        content (str): The content of the message.\n        tag (str): The tag to use, will show up as &lt;tag&gt;content&lt;/tag&gt;.\n        header (str): The header to reference the data\n\n    Returns:\n        message (Message): A user message with the data tagged.\n    \"\"\"\n    content = f\"{header}\\n\\n&lt;{tag}&gt;{content}&lt;/{tag}&gt;\"\n    return Message(content=content, role=MessageRole.USER)\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.user.TipsMessage","title":"<code>TipsMessage(tips, header='Here are some tips to help you complete the task')</code>","text":"<p>Create a system message that gives the user tips to help them complete the task.</p> <p>Parameters:</p> Name Type Description Default <code>tips</code> <code>List[str]</code> <p>A list of tips to help the user complete the task.</p> required <code>header</code> <code>str</code> <p>The header of the message.</p> <code>'Here are some tips to help you complete the task'</code> <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A user message that gives the user tips to help them complete the</p> Source code in <code>instructor/dsl/messages/user.py</code> <pre><code>def TipsMessage(\n    tips: List[str], header: str = \"Here are some tips to help you complete the task\"\n) -&gt; Message:\n    \"\"\"\n    Create a system message that gives the user tips to help them complete the task.\n\n    Parameters:\n        tips (List[str]): A list of tips to help the user complete the task.\n        header (str): The header of the message.\n\n    Returns:\n        message (Message): A user message that gives the user tips to help them complete the\n    \"\"\"\n    tips_str = \"\\n* \".join(tips)\n    return Message(\n        content=f\"{header}:\\n\\n* {tips_str}\",\n        role=MessageRole.USER,\n    )\n</code></pre>"},{"location":"chat-completion/#instructor.dsl.messages.user.UserMessage","title":"<code>UserMessage(content)</code>","text":"<p>Create a user message.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the message.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>Message</code> <p>A user message.</p> Source code in <code>instructor/dsl/messages/user.py</code> <pre><code>def UserMessage(content: str) -&gt; Message:\n    \"\"\"\n    Create a user message.\n\n    Parameters:\n        content (str): The content of the message.\n\n    Returns:\n        message (Message): A user message.\n    \"\"\"\n    return Message(content=content, role=MessageRole.USER)\n</code></pre>"},{"location":"distillation/","title":"Distilling python functions into LLM","text":"<p><code>Instructions</code> from the <code>Instructor</code> library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning <code>gpt-3.5-turbo</code> to emulate these functions end-to-end.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"distillation/#the-challenges-in-function-level-fine-tuning","title":"The Challenges in Function-Level Fine-Tuning","text":"<p>Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing <code>def f(a, b): return a * b</code>. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.</p>"},{"location":"distillation/#the-role-of-instructions-in-simplifying-the-fine-tuning-process","title":"The Role of <code>Instructions</code> in Simplifying the Fine-Tuning Process","text":"<p>By using <code>Instructions</code>, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.</p>"},{"location":"distillation/#how-to-implement-instructions-in-your-code","title":"How to Implement <code>Instructions</code> in Your Code","text":""},{"location":"distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs! \n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")]\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n</code></pre>"},{"location":"distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\"role\": \"assistant\", \n            \"function_call\": \n                {\n                    \"name\": \"Multiply\", \n                    \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}'\n            }\n        }\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ]\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre> <p>Once a model is trained you can simply change <code>mode</code> to <code>dispatch</code> and it will use the model to run the function!</p> <pre><code>from instructor import Instructions\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"maybe/","title":"Error Handling Using Maybe Pattern","text":""},{"location":"maybe/#introduction","title":"Introduction","text":"<p>The <code>Maybe</code> pattern is a functional programming concept used for error handling. Instead of raising exceptions or returning <code>None</code>, you can use a <code>Maybe</code> type to encapsulate both the result and possible errors.</p>"},{"location":"maybe/#define-models-with-pydantic","title":"Define Models with Pydantic","text":"<p>Using Pydantic, define the <code>UserDetail</code> and <code>MaybeUser</code> classes.</p> <pre><code>from pydantic import BaseModel, Field, Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre>"},{"location":"maybe/#implementing-maybe-pattern-with-instructor","title":"Implementing <code>Maybe</code> Pattern with <code>instructor</code>","text":"<p>You can use <code>instructor</code> to generalize the <code>Maybe</code> pattern.</p> <pre><code>import instructor\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre>"},{"location":"maybe/#function-example-get_user_detail","title":"Function Example: <code>get_user_detail</code>","text":"<p>Here's a function example that returns a <code>MaybeUser</code> instance. The function simulates an API call to get user details.</p> <pre><code>from typing import Optional\nimport random\n\ndef get_user_detail(string: str) -&gt; MaybeUser:\n    ...\n    return\n\n# Example usage\nuser1 = get_user_detail(\"Jason is a 25 years old scientist\")\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\n\nuser2 = get_user_detail(\"Unknown user\")\n{\n  \"result\": null,\n  \"error\": true,\n  \"message\": \"User not found\"\n}\n</code></pre>"},{"location":"maybe/#conclusion","title":"Conclusion","text":"<p>The <code>Maybe</code> pattern enables a more structured approach to error handling. This example illustrated its implementation using Python and Pydantic.</p>"},{"location":"multitask/","title":"Patterns for Extracting Multiple Items","text":"<p>A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction</p> <pre><code>class User(OpenAISchema):\n    name: str\n    age: int\n\nclass Users(OpenAISchema):\n    users: List[User]\n</code></pre> <p>Defining a task and creating a list of classes is a common enough pattern that we define a helper function <code>MultiTask</code> It procides a function to dynamically create a new class that:</p> <ol> <li>Dynamic docstrings and class name baed on the task</li> <li>Helper method to support streaming by collectin function_call tokens until a object back out.</li> </ol>"},{"location":"multitask/#extracting-tasks-using-multitask","title":"Extracting Tasks using MultiTask","text":"<p>By using multitask you get a very convient class with prompts and names automatically defined. You get <code>from_response</code> just like any other <code>OpenAISchema</code> you're able to extract the list of objects data you want with <code>MultTask.tasks</code>.</p> <pre><code>from instructor import OpenAISchema, MultiTask\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nMultiUser = MultiTask(User)\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-4-0613\",\n    temperature=0.1,\n    stream=False,\n    functions=[MultiUser.openai_schema],\n    function_call={\"name\": MultiUser.openai_schema[\"name\"]},\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider the data below: Jason is 10 and John is 30\",\n        },\n    ],\n    max_tokens=1000,\n)\nMultiUser.from_response(completion)\n</code></pre> <pre><code>{\"tasks\": [\n    {\"name\": \"Jason\", \"age\": 10},\n    {\"name\": \"John\", \"age\": 30}\n]}\n</code></pre>"},{"location":"multitask/#streaming-tasks","title":"Streaming Tasks","text":"<p>Since a <code>MultiTask(T)</code> is well contrained to <code>tasks: List[T]</code> we can make assuptions on how tokens are used and provide a helper method that allows you generate tasks as the the tokens are streamed in</p> <p>Why would we want this?</p> <p>While <code>gpt-3.5-turbo</code> is quite fast <code>gpt-4</code> will take a while if there are many objects or if each object schema is complex. If 10 entities are created and takes 100ms to complete it would mean that it would take 1 second before we had access to our objects. With streaming you'd get the first object in 100ms a 10x percieved improvement in latency! While this may not make sense for more usecases if we were dynamitcally building UI based on entities, streaming entities 1 by 1 could improve the user experience dramatically.</p> <p>Lets look at an example in action with the same class</p> <pre><code>MultiUser = MultiTask(User)\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-4-0613\",\n    temperature=0.1,\n    stream=True,\n    functions=[MultiUser.openai_schema],\n    function_call={\"name\": MultiUser.openai_schema[\"name\"]},\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in MultiUser.from_streaming_response(completion):\n    assert isinstance(user, User)\n    print(user)\n\n&gt;&gt;&gt; name=\"Jason\" \"age\"=10\n&gt;&gt;&gt; name=\"John\" \"age\"=10\n</code></pre> <p>How??</p> <p>Consider this incomplete json string.</p> <pre><code>{\"tasks\": [{\"name\": \"Jason\", \"age\": 10}\n</code></pre> <p>Notice how, while this isn't valid json, we know that one complete <code>User</code> object was generated so we <code>yield</code> that object to be used elsewhere as soon as possible.</p> <p>This streaming is still a prototype, but should work quite well for simple schemas.</p>"},{"location":"openai_schema/","title":"OpenAI Schema","text":"<p>The <code>OpenAISchema</code> is an extension of <code>Pydantic.BaseModel</code> that offers a minimally invasive way to define schemas for OpenAI completions. It provides two main methods: <code>openai_schema</code> to generate the correct schema and <code>from_response</code> to create an instance of the class from the completion result.</p>"},{"location":"openai_schema/#prompt-placement","title":"Prompt Placement","text":"<p>Our philosophy is to keep prompts close to the code. This is achieved by using docstrings and field descriptions to provide prompts and descriptions for your schema fields.</p>"},{"location":"openai_schema/#structured-extraction","title":"Structured Extraction","text":"<p>You can directly use the <code>OpenAISchema</code> class in your <code>openai</code> API create calls by passing in the <code>openai_schema</code> class property and extracting the class out using the <code>from_response</code> method. This style of usage provides full control over configuration and prompting.</p> <pre><code>import openai\nfrom instructor import OpenAISchema\nfrom pydantic import Field\n\nclass UserDetails(OpenAISchema):\n    \"\"\"Details of a user\"\"\"\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo-0613\",\n    functions=[UserDetails.openai_schema],\n    function_call={\"name\": UserDetails.openai_schema[\"name\"]},\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract user details from my requests\"},\n        {\"role\": \"user\", \"content\": \"My name is John Doe and I'm 30 years old.\"},\n    ],\n)\n\nuser_details = UserDetails.from_response(completion)\nprint(user_details)  # UserDetails(name='John Doe', age=30)\n</code></pre> <p>You can also use the <code>@openai_schema</code> decorator to decorate <code>BaseModels</code>, but you may lose some type hinting as a result.</p> <pre><code>import openai\nfrom instructor import openai_schema\nfrom pydantic import Field, BaseModel\n\n@openai_schema\nclass UserDetails(BaseModel):\n    \"\"\"Details of a user\"\"\"\n    name: str = Field(..., description=\"User's full name\")\n    age: int\n</code></pre>"},{"location":"openai_schema/#code-reference","title":"Code Reference","text":"<p>For more information about the code, including the complete API reference, please refer to the <code>instructor</code> documentation.</p>"},{"location":"openai_schema/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Augments a Pydantic model with OpenAI's schema for function calling</p> <p>This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.</p>"},{"location":"openai_schema/#instructor.function_calls.OpenAISchema--usage","title":"Usage","text":"<pre><code>from instructor import OpenAISchema\n\nclass User(OpenAISchema):\n    name: str\n    age: int\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo',\n    messages=[{\n        \"content\": \"Jason is 20 years old\",\n        \"role\": \"user\"\n    }],\n    functions=[User.openai_schema],\n    function_call={\"name\": User.openai_schema[\"name\"]},\n)\n\nuser = User.from_response(completion)\n\nprint(user.model_dump())\n</code></pre>"},{"location":"openai_schema/#instructor.function_calls.OpenAISchema--result","title":"Result","text":"<pre><code>{\n    \"name\": \"Jason Liu\",\n    \"age\": 20,\n}\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):\n    \"\"\"\n    Augments a Pydantic model with OpenAI's schema for function calling\n\n    This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call.\n\n    ## Usage\n\n    ```python\n    from instructor import OpenAISchema\n\n    class User(OpenAISchema):\n        name: str\n        age: int\n\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo',\n        messages=[{\n            \"content\": \"Jason is 20 years old\",\n            \"role\": \"user\"\n        }],\n        functions=[User.openai_schema],\n        function_call={\"name\": User.openai_schema[\"name\"]},\n    )\n\n    user = User.from_response(completion)\n\n    print(user.model_dump())\n    ```\n    ## Result\n\n    ```\n    {\n        \"name\": \"Jason Liu\",\n        \"age\": 20,\n    }\n    ```\n\n\n    \"\"\"\n\n    @classmethod\n    @property\n    def openai_schema(cls):\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classmethod\n    def from_response(\n        cls,\n        completion,\n        throw_error: bool = True,\n        validation_context=None,\n        strict: bool = None,\n    ):\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        message = completion[\"choices\"][0][\"message\"]\n\n        if throw_error:\n            assert \"function_call\" in message, \"No function call detected\"\n            assert (\n                message[\"function_call\"][\"name\"] == cls.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n\n        return cls.model_validate_json(\n            message[\"function_call\"][\"arguments\"],\n            context=validation_context,\n            strict=strict,\n        )\n</code></pre>"},{"location":"openai_schema/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema</code>  <code>classmethod</code> <code>property</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p>"},{"location":"openai_schema/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, throw_error=True, validation_context=None, strict=None)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> <code>True</code> <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion,\n    throw_error: bool = True,\n    validation_context=None,\n    strict: bool = None,\n):\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    message = completion[\"choices\"][0][\"message\"]\n\n    if throw_error:\n        assert \"function_call\" in message, \"No function call detected\"\n        assert (\n            message[\"function_call\"][\"name\"] == cls.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n\n    return cls.model_validate_json(\n        message[\"function_call\"][\"arguments\"],\n        context=validation_context,\n        strict=strict,\n    )\n</code></pre>"},{"location":"openai_schema/#instructor.function_calls.openai_function","title":"<code>openai_function</code>","text":"<p>Decorator to convert a function into an OpenAI function.</p> <p>This decorator will convert a function into an OpenAI function. The function will be validated using pydantic and the schema will be generated from the function signature.</p> Example <pre><code>@openai_function\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n\ncompletion = openai.ChatCompletion.create(\n    ...\n    messages=[{\n        \"content\": \"What is 1 + 1?\",\n        \"role\": \"user\"\n    }]\n)\nsum.from_response(completion)\n# 2\n</code></pre> Source code in <code>instructor/function_calls.py</code> <pre><code>class openai_function:\n    \"\"\"\n    Decorator to convert a function into an OpenAI function.\n\n    This decorator will convert a function into an OpenAI function. The\n    function will be validated using pydantic and the schema will be\n    generated from the function signature.\n\n    Example:\n        ```python\n        @openai_function\n        def sum(a: int, b: int) -&gt; int:\n            return a + b\n\n        completion = openai.ChatCompletion.create(\n            ...\n            messages=[{\n                \"content\": \"What is 1 + 1?\",\n                \"role\": \"user\"\n            }]\n        )\n        sum.from_response(completion)\n        # 2\n        ```\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self.func = func\n        self.validate_func = validate_arguments(func)\n        self.docstring = parse(self.func.__doc__ or \"\")\n\n        parameters = self.validate_func.model.model_json_schema()\n        parameters[\"properties\"] = {\n            k: v\n            for k, v in parameters[\"properties\"].items()\n            if k not in (\"v__duplicate_kwargs\", \"args\", \"kwargs\")\n        }\n        for param in self.docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                parameters[\"properties\"][name][\"description\"] = description\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n        self.openai_schema = {\n            \"name\": self.func.__name__,\n            \"description\": self.docstring.short_description,\n            \"parameters\": parameters,\n        }\n        self.model = self.validate_func.model\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        @wraps(self.func)\n        def wrapper(*args, **kwargs):\n            return self.validate_func(*args, **kwargs)\n\n        return wrapper(*args, **kwargs)\n\n    def from_response(self, completion, throw_error=True, strict: bool = None):\n        \"\"\"\n        Parse the response from OpenAI's API and return the function call\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from OpenAI's API\n            throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n        Returns:\n            result (any): result of the function call\n        \"\"\"\n        message = completion[\"choices\"][0][\"message\"]\n\n        if throw_error:\n            assert \"function_call\" in message, \"No function call detected\"\n            assert (\n                message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n            ), \"Function name does not match\"\n\n        function_call = message[\"function_call\"]\n        arguments = json.loads(function_call[\"arguments\"], strict=strict)\n        return self.validate_func(**arguments)\n</code></pre>"},{"location":"openai_schema/#instructor.function_calls.openai_function.from_response","title":"<code>from_response(completion, throw_error=True, strict=None)</code>","text":"<p>Parse the response from OpenAI's API and return the function call</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from OpenAI's API</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the response does not contain a function call</p> <code>True</code> <p>Returns:</p> Name Type Description <code>result</code> <code>any</code> <p>result of the function call</p> Source code in <code>instructor/function_calls.py</code> <pre><code>def from_response(self, completion, throw_error=True, strict: bool = None):\n    \"\"\"\n    Parse the response from OpenAI's API and return the function call\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from OpenAI's API\n        throw_error (bool): Whether to throw an error if the response does not contain a function call\n\n    Returns:\n        result (any): result of the function call\n    \"\"\"\n    message = completion[\"choices\"][0][\"message\"]\n\n    if throw_error:\n        assert \"function_call\" in message, \"No function call detected\"\n        assert (\n            message[\"function_call\"][\"name\"] == self.openai_schema[\"name\"]\n        ), \"Function name does not match\"\n\n    function_call = message[\"function_call\"]\n    arguments = json.loads(function_call[\"arguments\"], strict=strict)\n    return self.validate_func(**arguments)\n</code></pre>"},{"location":"philosophy/","title":"Philosophy","text":"<p>The philosophy behind this library is to provide a lightweight and flexible approach to leveraging language models (LLMs) to do structured output without imposing unnecessary dependencies or abstractions.</p> <p>The <code>instructor</code> library serves as a bridge from text-based language model interaction to Object-Oriented Programming, seamlessly integrating LLMs into the programming paradigms we're familiar with. By treating LLMs as callable functions that return typed objects, <code>instructor</code> demystifies their complexity, making them more accessible for everyday projects. This approach maintains the flexibility and power of Python, letting you write custom code without unnecessary constraints.</p> <ol> <li>Define a Schema <code>class StructuredData(BaseModel):</code></li> <li>Encapsulate all your LLM logic into a function <code>def extract(a) -&gt; StructuredData:</code> </li> <li>Define typed computations against your data with <code>def compute(data: StructuredData):</code></li> </ol> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements.</p> <p>If you have any further questions or ideas hit me up on twitter</p>"},{"location":"reask/","title":"Reasking When Validation Fails","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask. This allows the client to reattempt the API call a specified number of times if validation fails. Its another layer of defense against bad outputs of two forms.</p> <ol> <li>Pydantic Validation Errors</li> <li>JSON Decoding Errors</li> </ol>"},{"location":"reask/#future-improvements","title":"Future Improvements","text":"<p>Contributions Welcome</p> <p>The current retry mechanism relies on a while loop. For a more robust solution, contributions to integrate the <code>tenacity</code> library are welcome.</p>"},{"location":"reask/#example-using-validators-for-reasking","title":"Example: Using Validators for Reasking","text":"<p>The example utilizes Pydantic's field validators in tandem with the <code>max_retries</code> parameter. In this example if the <code>name</code> field fails validation, the <code>openai</code> client will reattempt the API call. Here we use a plain validator, but we can also use llms for validation</p>"},{"location":"reask/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<pre><code>import instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\ninstructor.patch()\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>Here, the <code>UserDetails</code> class includes a validator for the <code>name</code> attribute. The validator checks that the name is in uppercase and raises a <code>ValueError</code> otherwise.</p>"},{"location":"reask/#step-2-exception-handling-and-reasking","title":"Step 2: Exception Handling and Reasking","text":"<p>When validation fails, several steps are taken:</p> <ol> <li>The existing messages are retained for the new API request.</li> <li>The previous function call's response is added back.</li> <li>A user prompt is included to reask the model, with details on the error.</li> </ol> <pre><code>try:\n    ...\nexcept (ValidationError, JSONDecodeError) as e:\n    kwargs[\"messages\"].append(dict(**response.choices[0].message))\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"reask/#using-the-client-with-retries","title":"Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>model = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p>"},{"location":"reask/#takeaways","title":"Takeaways","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the systen can use to heal. This approach leverages existing programming practices for error handling, avoiding the need for new methodologies. We simplify the issue into code we already know how to write and leverage pydantic's powerful validation system to do so.</p>"},{"location":"reask_validation/","title":"Integrated Validation and Reask with LLMs and Pydantic","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the systen can use to self heal.</p>"},{"location":"reask_validation/#applications-and-scenarios","title":"Applications and Scenarios","text":"<ul> <li>Content Moderation: LLMs can be trained or guided to recognize and filter out objectionable or sensitive material, ensuring a safer user experience.</li> <li>Reflecting on Chain of Thought: As LLMs can evaluate their own reasoning process, this opens doors to even more reliable and dependable automated systems.</li> <li>Verifying Hallucinations: LLMs can be configured to recognize when they generate data or responses that do not align with facts or reliable data, reducing the risk of disseminating false information.</li> <li>Data Integrity: Enforces data quality standards.</li> </ul>"},{"location":"reask_validation/#pythonic-validation-with-pydantic-and-instructor","title":"Pythonic Validation with Pydantic and Instructor","text":"<ol> <li>Uniform Validation API: Pydantic provides identical developer experience, whether using code-based or LLM-based validation.</li> <li>Reasking Mechanism: Pydantic accumulates validation errors for a one-step reasking process.</li> <li>Prompt Chaining via Error Messages: Instructor utilizes validation error messages to refine LLM outputs without and new abstractions.</li> </ol>"},{"location":"reask_validation/#uniform-validation-code-based-vs-llm","title":"Uniform Validation: Code-Based vs. LLM","text":"<p>Validation is crucial when using Large Language Models (LLMs) for data extraction. It ensures data integrity, ensuring both quantitative and qualititave correctness with code and llm validations.</p> <p>Pydantic Validation Docs</p> <p>Pydantic supports validation individual fields or the whole model dict all at once.</p> <ul> <li>Field-Level Validation</li> <li>Model-Level Validation</li> </ul> <p>To see the most up to date examples check out our repo jxnl/instructor/examples/validators</p>"},{"location":"reask_validation/#code-based-validation-example","title":"Code-Based Validation Example","text":"<p>Model Level Evaluation</p> <p>Right now we only go over the field level examples, check out Model-Level Validation if you want to see how to do model level evaluation</p> <p>Enforce a naming rule using Pydantic's built-in validation:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n</code></pre>"},{"location":"reask_validation/#output-for-code-based-validation","title":"Output for Code-Based Validation","text":"<pre><code>1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n</code></pre>"},{"location":"reask_validation/#llm-based-validation-example","title":"LLM-Based Validation Example","text":"<p>LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>from pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instruct import llm_validator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str, \n        BeforeValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>"},{"location":"reask_validation/#output-for-llm-based-validation","title":"Output for LLM-Based Validation","text":"<p>Its important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"reask_validation/#using-reasking-logic-to-correct-outputs","title":"Using Reasking Logic to Correct Outputs","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask the model to correct the output.</p> <p>Its a great layer of defense against bad outputs of two forms.</p> <ol> <li>Pydantic Validation Errors (code or llm based)</li> <li>JSON Decoding Errors (when the model returns a bad response)</li> </ol>"},{"location":"reask_validation/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<p>Noticed the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a <code>ValueError</code> if the name is not in uppercase.</p> <pre><code>import instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\ninstructor.patch()\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre>"},{"location":"reask_validation/#step-2-using-the-client-with-retries","title":"Step 2. Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>model = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre>"},{"location":"reask_validation/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>Behind the scenes, the <code>instructor.patch()</code> method adds a <code>max_retries</code> parameter to the <code>openai.ChatCompletion.create()</code> method.  The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p> <pre><code>try:\n    ...\nexcept (ValidationError, JSONDecodeError) as e:\n    kwargs[\"messages\"].append(dict(**response.choices[0].message))\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"reask_validation/#advanced-validation-techniques","title":"Advanced Validation Techniques","text":"<p>The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better, for a example of model level validation, and using a validation context check out our example on verifying citations which covers</p> <ol> <li>Validate the entire object with all attributes rather than one attribute at a time</li> <li>Using some 'context' to validate the object, in this case we use the <code>context</code> to check if the citation existed in the original text.</li> </ol>"},{"location":"reask_validation/#takeaways","title":"Takeaways","text":"<p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content but also pave the way for more autonomous and effective systems.</p>"},{"location":"validation/","title":"Introduction to Validation in Pydantic and LLMs","text":"<p>Validation is crucial when using Large Language Models (LLMs) for data extraction. It ensures data integrity, enables reasking for better results, and allows for overwriting incorrect values. Pydantic offers versatile validation capabilities suitable for use with LLM outputs.</p> <p>Pydantic Validation Docs</p> <p>Pydantic supports validation individual fields or the whole model dict all at once.</p> <ul> <li>Field-Level Validation</li> <li>Model-Level Validation</li> </ul> <p>To see the most up to date examples check out our repo jxnl/instructor/examples/validators</p>"},{"location":"validation/#importance-of-llm-validation","title":"Importance of LLM Validation","text":"<ul> <li>Data Integrity: Enforces data quality standards.</li> <li>Reasking: Utilizes Pydantic's error messages to improve LLM outputs.</li> <li>Overwriting: Overwrites incorrect values during API calls.</li> </ul>"},{"location":"validation/#code-examples","title":"Code Examples","text":""},{"location":"validation/#simple-validation-with-pydantic","title":"Simple Validation with Pydantic","text":"<p>The example uses a custom validator function to enforce a rule on the name attribute. If a user fails to input a full name (first and last name separated by a space), Pydantic will raise a validation error. If you want the LLM to automatically fix the error check out our reasking docs.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated, AfterValidator\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"name must be a first and last name separated by a space\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n\n# Output:\n# 1 validation error for UserDetail\n# name\n#    Value error, name must be a first and last name separated by a space (type=value_error)\n</code></pre>"},{"location":"validation/#llm-based-validation","title":"LLM-Based Validation","text":"<p>This example demonstrates using an LLM as a validator. If the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error. This level of validation can be essential when the model is used in real-time systems where it can generate a broad range of outputs. Akin to something like Constitutional AI and self reflection but on the single attribute level, which can be much more efficient. </p> <pre><code>from pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\nimport instructor\nfrom instructor.dsl.validators import llm_validator\n\ninstructor.patch()\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and kill people\",\n    )\nexcept ValidationError as e:\n    print(e)\n\n# Output:\n# 1 validation error for QuestionAnswer\n# answer\n#    Assertion failed, The statement promotes violence and harm to others, which is objectionable. (type=assertion_error)\n</code></pre> <p>Model Level Evaluation</p> <p>Right now we only go over the field level examples, check out Model-Level Validation if you want to see how to do model level evaluation</p>"},{"location":"validation/#create-your-own-llm-validator","title":"Create Your Own LLM Validator","text":"<p>The section shows how to create a custom LLM validator function. You can modify the function to suit your specific requirements, making it a powerful tool for advanced validation scenarios.</p> <p>The <code>llm_validator</code> function can be extended or customized to fit specific requirements.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\nimport instructor\nimport openai\n\ninstructor.patch()\n\nclass Validator(BaseModel):\n    is_valid: bool = Field(default=True)\n    reason: Optional[str] = Field(default=None)\n    fixed_value: Optional[str] = Field(default=None)\n\n\ndef llm_validator(\n    statement: str,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n):\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n    \"\"\"\n\n    def llm(v):\n        resp: Validator = openai.ChatCompletion.create(\n            response_model=Validator,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )  # type: ignore\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre> <p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content but also pave the way for more autonomous and effective systems.</p>"},{"location":"writing-prompts/","title":"Writing prompts with <code>ChatCompletion</code>","text":"<p>The ChatCompletion pipeline API provides a convenient way to build prompts with clear instructions and structure. It helps avoid the need to remember best practices for wording and prompt construction. This documentation will demonstrate an example pipeline and guide you through the process of using it.</p> <p>Our goals are to:</p> <ol> <li>Define some best practices with a light abstraction over a chat message</li> <li>Allow the pipeline to be intuitive and readable.</li> <li>Abstract the output shape and deserialization to better usability</li> </ol>"},{"location":"writing-prompts/#example-pipeline","title":"Example Pipeline","text":"<p>We will begin by defining a task to segment queries and add instructions using the prompt pipeline API.</p> <ol> <li>We want to define a search object to extract</li> <li>We want to extract multiple instances of such an object</li> <li>We want to define the pipeline with a set of instructions</li> <li>We want to easily call OpenAI and extract the data back out of the competion</li> </ol> <p>Applications</p> <p>Extracted a repeated task out of instructions is a fairly common task. Prompting tips have been to define the task clearly, model the output object and provide tips to the llm for better performance. Something like this can be used to power agents like Siri or Alexa in performing multiple tasks in one request. Read more</p>"},{"location":"writing-prompts/#designing-the-schema","title":"Designing the Schema","text":"<p>First, let's design the schema for our task. In this example, we will have a <code>SearchQuery</code> schema with a single field called <code>query</code>. The <code>query</code> field will represent a detailed, comprehensive, and specific query to be used for semantic search.</p> <pre><code>from instructor import OpenAISchema, dsl\nfrom pydantic import Field\n\nclass SearchQuery(OpenAISchema):\n    query: str = Field(\n        ...,\n        description=\"Detailed, comprehensive, and specific query to be used for semantic search\",\n    )\n\nSearchResponse = dsl.MultiTask(\n    subtask_class=SearchQuery,\n)\n</code></pre> <p>MultiTask</p> <p>To learn more about the <code>MultiTask</code> functionality, you can refer to the MultiTask documentation.</p>"},{"location":"writing-prompts/#building-our-prompts","title":"Building our Prompts","text":"<p>Next, let's write out prompt using the pipeline style. We will leverage the features provided by the <code>ChatCompletion</code> class and utilize the <code>|</code> operator to chain different components of our prompt together.</p> <pre><code>task = (\n    dsl.ChatCompletion(#(1)!\n        name=\"Segmenting Search requests example\",\n        model='gpt-3.5-turbo-0613,\n        max_token=1000) \n    | dsl.SystemTask(task=\"Segment search results\") #(2)!\n    | dsl.TaggedMessage(#(3)!\n        content=\"can you send me the data about the video investment and the one about spot the dog?\",\n        tag=\"query\") \n    | dsl.TipsMessage(#(4)!\n        tips=[\n            \"Expand query to contain multiple forms of the same word (SSO -&gt; Single Sign On)\",\n            \"Use the title to explain what the query should return, but use the query to complete the search\",\n            \"The query should be detailed, specific, and cast a wide net when possible\",\n        ]) \n    | SearchResponse #(5)!\n)\n</code></pre> <ol> <li>Define the completion object (consider this both task and prompt)</li> <li>SystemTask augments the <code>task</code> with \"You are a world class ... correctly complete the task: {task}\"</li> <li>TaggedMessage wraps content with <code>&lt;query&gt;&lt;/query&gt;</code> to set clear boundaries for the data you wish to process</li> <li>TipsMessages allows you to pass a list of strings as tips as a result we can potentially create this list dynamically</li> <li>Last step defines the output model you want to use to parse the results if no output model is defined we revert to the usual openai completion.</li> </ol> <p>The <code>ChatCompletion</code> class is responsible for model configuration, while the <code>|</code> operator allows us to construct the prompt in a readable manner. We can add <code>Messages</code> or <code>OpenAISchema</code> components to the prompt pipeline using <code>|</code>, and the <code>ChatCompletion</code> class will handle the prompt construction for us.</p> <p>In the above example, we:</p> <ul> <li>Initialize a <code>ChatCompletion</code> object with the desired model and maximum token count.</li> <li>Add a <code>SystemTask</code> component to segment search results.</li> <li>Include a <code>TaggedMessage</code> component to provide a query with a specific tag.</li> <li>Use a <code>TipsMessage</code> component to include some helpful tips related to the task.</li> <li>Connect the <code>SearchResponse</code> schema to the pipeline.</li> </ul> <p>Lastly, we create the <code>search_request</code> using <code>task.create()</code>. The <code>search_request</code> object will be of type <code>SearchResponse</code>, and we can print it as a JSON object.</p> <p>Tip</p> <p>If you want to see the exact input sent to OpenAI, scroll to the bottom of the page.</p> <pre><code>search_request = task.create()  # type: ignore\nassert isinstance(search_request, SearchResponse)\nprint(search_request.json(indent=2))\n</code></pre> <p>The output will be a JSON object containing the segmented search queries.</p> <pre><code>{\n  \"tasks\": [\n    {\n      \"query\": \"data about video investment\"\n    },\n    {\n      \"query\": \"data about spot the dog\"\n    }\n  ]\n}\n</code></pre>"},{"location":"writing-prompts/#inspecting-the-api-call","title":"Inspecting the API Call","text":"<p>To make it easy for you to understand what this api is doing we default only construct the kwargs for the chat completion call.</p> <pre><code>print(task.kwargs)\n</code></pre> <pre><code>{\n \"messages\": [\n  {\n   \"role\": \"system\",\n   \"content\": \"You are a world class state of the art algorithm capable of correctly completing the following task: `Segment search results`.\"\n  },\n  {\n   \"role\": \"user\",\n   \"content\": \"Consider the following data:\\n\\n&lt;query&gt;can you send me the data about the video investment and the one about spot the dog?&lt;/query&gt;\"\n  },\n  {\n   \"role\": \"user\",\n   \"content\": \"Here are some tips to help you complete the task:\\n\\n* Expand query to contain multiple forms of the same word (SSO -&gt; Single Sign On)\\n* Use the title to explain what the query should return, but use the query to complete the search\\n* The query should be detailed, specific, and cast a wide net when possible\"\n  }\n ],\n \"functions\": [\n  {\n   \"name\": \"MultiSearchQuery\",\n   \"description\": \"Correctly segmented set of search queries\",\n   \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n     \"tasks\": {\n      \"description\": \"Correctly segmented list of `SearchQuery` tasks\",\n      \"type\": \"array\",\n      \"items\": {\n       \"$ref\": \"#/definitions/SearchQuery\"\n      }\n     }\n    },\n    \"definitions\": {\n     \"SearchQuery\": {\n      \"type\": \"object\",\n      \"properties\": {\n       \"query\": {\n        \"description\": \"Detailed, comprehensive, and specific query to be used for semantic search\",\n        \"type\": \"string\"\n       }\n      },\n      \"required\": [\n       \"query\"\n      ]\n     }\n    },\n    \"required\": [\n     \"tasks\"\n    ]\n   }\n  }\n ],\n \"function_call\": {\n  \"name\": \"MultiSearchQuery\"\n },\n \"max_tokens\": 1000,\n \"temperature\": 0.1,\n \"model\": \"gpt-3.5-turbo-0613\"\n}\n</code></pre>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/","title":"Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation","text":"","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#introduction","title":"Introduction","text":"<p>Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the <code>instructor.instructions</code> streamlines this process, making the task you want to distil  more efficient and powerful while preserving its original functionality and backwards compatibility.</p> <p>If you want to see the full example checkout examples/distillation</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-use-instructor","title":"Why use Instructor?","text":"<p>Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where <code>Instructor</code> comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs! \n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")]\n)\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\"role\": \"assistant\", \n            \"function_call\": \n                {\n                    \"name\": \"Multiply\", \n                    \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}'\n            }\n        }\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ]\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#next-steps-and-future-plans","title":"Next Steps and Future Plans","text":"<p>Here's a sneak peek of what I'm planning:</p> <pre><code>from instructor import Instructions\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#conclusion","title":"Conclusion","text":"<p>We've seen how <code>Instructor</code> can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["python","distillation","function calling","finetuning"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/","title":"Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls","text":"<p>Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-problem-with-existing-llm-frameworks","title":"The Problem with Existing LLM Frameworks","text":"<p>Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-openai-function-calling-game-changer","title":"The OpenAI Function Calling Game-Changer","text":"<p>OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#why-pydantic","title":"Why Pydantic?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and the language model.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul> <pre><code>import pydantic\nimport instructor\nimport openai\n\n# Enables the response_model\ninstructor.patch()\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\nuser: UserDetail = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#simplifying-validation-flow-with-pydantic","title":"Simplifying Validation Flow with Pydantic","text":"<p>Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator, patch\n\nimport openai\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\")\n        ),\n    ]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#the-modular-approach","title":"The Modular Approach","text":"<p>Pydantic allows for modular output schemas. This leads to more organized code.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#composition-of-schemas","title":"Composition of Schemas","text":"<pre><code>class UserDetails(BaseModel):\n    name: str\n    age: int\n\nclass UserWithAddress(UserDetails):\n    address: str\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#defining-relationships","title":"Defining Relationships","text":"<pre><code>class UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-enums","title":"Using Enums","text":"<pre><code>from enum import Enum, auto\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#flexible-schemas","title":"Flexible Schemas","text":"<pre><code>from typing import List\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#chain-of-thought","title":"Chain of Thought","text":"<pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#language-models-as-microservices","title":"Language Models as Microservices","text":"<p>The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains.</p>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#fastapi-stub","title":"FastAPI Stub","text":"<pre><code>app = FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -&gt; UserDetails:\n    return UserDetails(...)\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#using-instructor-as-a-function","title":"Using Instructor as a Function","text":"<pre><code>def extract_user(str) -&gt; UserDetails:\n    return openai.ChatCompletion(\n           response_model=UserDetails,\n           messages=[...]\n    )\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#response-modeling","title":"Response Modeling","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n</code></pre>","tags":["Introduction"]},{"location":"blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/#conclusion","title":"Conclusion","text":"<p>Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["Introduction"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems,  and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. </p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. </p> <pre><code>import instructor \nimport openai\n\n# Enables response_model in the openai client\ninstructor.patch()\n\nquery = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\", \n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"What are some recent developments in AI?\"\n        }\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n    \"published_daterange\": {\n        \"start\": \"2023-09-17\",\n        \"end\": \"2021-06-17\"\n    },\n    \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\nclass Retrival(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor \nimport openai\n\n# Enables response_model in the openai client\ninstructor.patch()\n\nretrival = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    response_model=Retrival,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"}\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providors and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#conclusion","title":"Conclusion","text":"<p>This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>I believe collaboration between domain experts and AI engineers is the key to enable advanced tool use. I\u2019ve been building a new tool on top of instructor that enables seamless collaboration and experimentation on LLMs with structured outputs. If you\u2019re interested, visit useinstructor.com and take our survey to join the waitlist.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>","tags":["RAG","Embeddings","Query Understanding","Search Systems","Personal Assistant"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/","title":"Good LLM Validation is Just Good Validation","text":"<p>What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.</p> <p>Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like <code>Pydantic</code> and <code>Instructor</code>. We validate these outputs using a validation function which conforms to the structure seen below.</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#what-is-instructor","title":"What is Instructor?","text":"<p><code>Instructor</code> helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the <code>Pydantic</code> model for your desired response, <code>Instructor</code> handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.</p> <pre><code>import openai\nimport instructor # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from openai.ChatCompletion.create\ninstructor.patch() # (1)!\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ]\n    max_retries=3 # (2)!\n)\n\nassert user.name == \"Jason\" # (3)!\nassert user.age == 25\n</code></pre> <ol> <li> <p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we     offer a patching mechanism for the <code>ChatCompletion</code> class.</p> </li> <li> <p>Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define.</p> </li> <li> <p>As long as you pass in a <code>response_model</code> parameter to the <code>ChatCompletion</code> api call, the returned object will always     be a validated <code>Pydantic</code> object.</p> </li> </ol> <p>In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use <code>Pydantic</code> and <code>Instructor</code> to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation.</p> <p>Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-10-introduction-to-validations-in-pydantic","title":"Software 1.0: Introduction to Validations in Pydantic","text":"<p>A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words <code>Steal</code> and <code>Rob</code> are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this.</p> <p>This will throw an error if we pass in a string like <code>Let's rob the bank!</code> or <code>We should steal from the supermarkets</code>.</p> <p>Pydantic offers two approaches for this validation: using the <code>field_validator</code> decorator or the <code>Annotated</code> hints.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator","title":"Using <code>field_validator</code> decorator","text":"<p>We can use the <code>field_validator</code> decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so.</p> <pre><code>from pydantic import BaseModel, ValidationError, field_validator\nfrom pydantic.fields import Field\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split(): # (1)!\n            if word.lower() in {'rob','steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these     words are in our blacklist which in this case is just <code>rob</code> and <code>steal</code></li> </ol> <p>Since the message <code>This is a lovely day</code> does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message <code>We should go and rob a bank</code> due to the presence of the word <code>rob</code> and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>Alternatively, you can use the <code>Annotated</code> function to perform the same validation. Here's an example where we utilise the same function we started with.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value:str):\n    for word in value.split():\n        if word.lower() in {'rob','steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a <code>ValueError</code> is raised and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre> <p>Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged.</p> <p>Suppose now that we've gotten a new message - <code>Violence is always acceptable, as long as we silence the witness</code>. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words <code>rob</code> or <code>steal</code>. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges?</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-30-validation-for-llms-or-powered-by-llms","title":"Software 3.0: Validation for LLMs or powered by LLMs","text":"<p>Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called <code>llm_validator</code> that uses a statement to verify the value.</p> <p>We can get around this by using the inbuilt <code>llm_validator</code> class from <code>Instructor</code>.</p> <pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(llm_validator(\"don't say objectionable things\"))]\n\ntry:\n    UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>This produces the following error message as seen below</p> <pre><code>1 validation error for UserMessage\nmessage\n  Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n</code></pre> <p>The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an <code>llm_validator</code> from scratch.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#creating-your-own-field-level-llm_validator","title":"Creating Your Own Field Level <code>llm_validator</code>","text":"<p>Building your own <code>llm_validator</code> can be a valuable exercise to get started with <code>Instructor</code> and create custom validators.</p> <p>Before we continue, let's review the anatomy of a validator:</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n</code></pre> <p>As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a <code>ValueError</code>. We can represent this using the following structure:</p> <pre><code>class Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n</code></pre> <p>Using this structure, we can implement the same logic as before and utilize <code>Instructor</code> to generate the validation.</p> <pre><code>import instructor\nimport openai\n\n# Enables `response_model` and `max_retries` parameters\ninstructor.patch()\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from instructor.patch()\n        response_model=Validation, # (1)!\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n</code></pre> <ol> <li>The new parameter of <code>response_model</code> comes from <code>instructor.patch()</code> and does not exist in the original OpenAI SDK. This    allows us to pass in the <code>Pydantic</code> model that we want as a response.</li> </ol> <p>Now we can use this validator in the same way we used the <code>llm_validator</code> from <code>Instructor</code>.</p> <pre><code>class UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(validator)]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#writing-more-complex-validations","title":"Writing more complex validations","text":"","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-chain-of-thought","title":"Validating Chain of Thought","text":"<p>A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt.</p> <p>We can utilise <code>Pydantic</code> and <code>Instructor</code> to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator.</p> <pre><code>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from instructor.patch()\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</code></pre> <p>We can then take advantage of the <code>model_validator</code> decorator to perform a validation on a subset of the model's data.</p> <p>We're defining a model validator here which runs before <code>Pydantic</code> parses the input into its respective fields. That's why we have a before keyword used in the <code>model_validator</code> class.</p> <pre><code>from pydantic import BaseModel, model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</code></pre> <p>Now, when you create a <code>AIResponse</code> instance, the <code>chain_of_thought_makes_sense</code> validator will be invoked. Here's an example:</p> <pre><code>try:\n    resp = AIResponse(\n        chain_of_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\"\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>If we create a <code>AIResponse</code> instance with an answer that does not follow the chain of thought, we will get an error.</p> <pre><code>1 validation error for AIResponse\n    Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2.\n    [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-citations-from-original-text","title":"Validating Citations From Original Text","text":"<p>Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically.</p> <p>We can pass in additional context to our validation functions using the <code>model_validate</code> function in <code>Pydantic</code> so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the <code>info</code> argument in our validator functions.</p> <pre><code>from pydantic import ValidationInfo,BaseModel,field_validator\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo): # (1)!\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n</code></pre> <ol> <li>This <code>info</code> object corresponds to the value of <code>context</code> that we pass into the <code>model_validate</code> function as seen below.</li> </ol> <p>We can then take our original example and test it against our new model</p> <pre><code>try:\n    AnswerWithCitation.model_validate(\n        {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"},\n        context={\"text_chunk\": \"Jason is just a guy\"}, # (1)!\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>This <code>context</code> object is just a normal python dictionary and can take in and store any arbitrary values</li> </ol> <p>This in turn generates the following error since <code>Jason is cool</code> does not exist in the text <code>Jason is just a guy</code>.</p> <pre><code>1 validation error for AnswerWithCitation\ncitation\nValue error, Citation `Jason is cool` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#putting-it-all-together-with-instructorpatch","title":"Putting it all together with <code>instructor.patch()</code>","text":"<p>To pass this context from the <code>openai.ChatCompletion.create</code> call, <code>instructor.patch()</code> also passes the <code>validation_context</code>, which will be accessible from the <code>info</code> argument in the decorated validator functions.</p> <pre><code>import openai\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\ninstructor.patch()\n\ndef answer_question(question:str, text_chunk: str) -&gt; AnswerWithCitation:\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n</code></pre>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#error-handling-and-re-asking","title":"Error Handling and Re-Asking","text":"<p>Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running <code>instructor.patch()</code> not only do we add <code>response_model</code> and <code>validation_context</code> it also allows you to use the <code>max_retries</code> parameter to specify the number of times to try and self correct.</p> <p>This approach provides a layer of defense against two types of bad outputs:</p> <ol> <li>Pydantic Validation Errors (code or LLM-based)</li> <li>JSON Decoding Errors (when the model returns an incorrect response)</li> </ol>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#define-the-response-model-with-validators","title":"Define the Response Model with Validators","text":"<p>To keep things simple lets assume we have a model that returns a <code>UserModel</code> object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase.</p> <pre><code>from pydantic import BaseModel, field_validator\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>This is where the <code>max_retries</code> parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt.</p> <pre><code>model = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n    # Powered by instructor.patch()\n    response_model=UserModel,\n    max_retries=2,\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output.</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#conclusion","title":"Conclusion","text":"<p>From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it.</p> <p>If you enjoy the content or want to try out <code>Instructor</code> please check out the github and give us a star!</p>","tags":["pydantic","validation","guardrails","constitutional ai","chain of thought","citations"]},{"location":"cli/","title":"Instructor CLI","text":"<p>Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services.</p> <p>You can set the API key in your terminal as follows:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"cli/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>pip install instructor\n</code></pre>"},{"location":"cli/#features","title":"Features","text":"<ul> <li> <p>API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide.</p> </li> <li> <p>Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide.</p> </li> </ul>"},{"location":"cli/#support-contribution","title":"Support &amp; Contribution","text":"<p>Need help or want to contribute? Visit our GitHub Repository</p>"},{"location":"cli/finetune/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI.</p>"},{"location":"cli/finetune/#creating-a-fine-tuning-job","title":"Creating a Fine-Tuning Job","text":""},{"location":"cli/finetune/#view-jobs-options","title":"View Jobs Options","text":"<pre><code>$ instructor jobs --help \n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...                                                            \n\n Monitor and create fine tuning jobs                                                                           \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel                    Cancel a fine-tuning job.                                                         \u2502\n\u2502 create-from-file          Create a fine-tuning job from a file.                                             \u2502\n\u2502 create-from-id            Create a fine-tuning job from an existing ID.                                     \u2502\n\u2502 list                      Monitor the status of the most recent fine-tuning jobs.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#create-from-file","title":"Create from File","text":"<p>The create-from-file command uploads and trains a model in a single step:</p> <pre><code>$ instructor jobs create-from-file transformed_data.jsonl \n</code></pre>"},{"location":"cli/finetune/#create-from-id","title":"Create from ID","text":"<p>The create-from-id command uses an uploaded file and trains a model</p> <pre><code>$ instructor files upload transformed_data.jsonl \n$ instructor files list\n...\n$ instructor jobs create-from-file &lt;file_id&gt;\n</code></pre>"},{"location":"cli/finetune/#viewing-files-and-jobs","title":"Viewing Files and Jobs","text":""},{"location":"cli/finetune/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>$ instructor jobs list \n\nOpenAI Fine Tuning Job Monitoring                                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                \u2503              \u2503                \u2503     Completion \u2503                 \u2503                \u2503        \u2503                 \u2503\n\u2503 Job ID         \u2503 Status       \u2503  Creation Time \u2503           Time \u2503 Model Name      \u2503 File ID        \u2503 Epochs \u2503 Base Model      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ftjob-PWo6uwk\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       23:10:54 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-1whjva8\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:47:05 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-wGoBDld\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:44:12 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-yd5aRTc\u2026 \u2502 \u2705 succeeded \u2502     2023-08-23 \u2502     2023-08-23 \u2502 ft:gpt-3.5-tur\u2026 \u2502 file-IQxAUDqX\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       14:26:03 \u2502       15:02:29 \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n</code></pre>"},{"location":"cli/finetune/#viewing-files","title":"Viewing Files","text":"<pre><code>$ instructor files list \n\nOpenAI Files                                                      \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513                         \n\u2503 File ID                       \u2503 Size (bytes) \u2503 Creation Time       \u2503 Filename \u2503 Purpose   \u2503                         \n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529                         \n\u2502 file-0lw2BSNRUlXZXRRu2beCCWjl \u2502       369523 \u2502 2023-08-23 23:31:57 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-IHaUXcMEykmFUp1kt2puCDEq \u2502       369523 \u2502 2023-08-23 23:09:35 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-ja9vRBf0FydEOTolaa3BMqES \u2502       369523 \u2502 2023-08-23 22:42:29 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-F7lJg6Z47CREvmx4kyvyZ6Sn \u2502       369523 \u2502 2023-08-23 22:42:03 \u2502 file     \u2502 fine-tune \u2502                         \n\u2502 file-YUxqZPyJRl5GJCUTw3cNmA46 \u2502       369523 \u2502 2023-08-23 22:29:10 \u2502 file     \u2502 fine-tune \u2502                         \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \n</code></pre>"},{"location":"cli/finetune/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"cli/usage/","title":"Using the OpenAI API Usage CLI","text":"<p>The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost.</p>"},{"location":"cli/usage/#monitoring-api-usage","title":"Monitoring API Usage","text":""},{"location":"cli/usage/#view-usage-options","title":"View Usage Options","text":"<pre><code>$ instructor usage --help\n\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...                                                           \n\n Check OpenAI API usage data                                                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 list       Displays OpenAI API usage data for the past N days.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/usage/#list-usage-for-specific-number-of-days","title":"List Usage for Specific Number of Days","text":"<p>To display API usage for the past 3 days, use the following command:</p> <pre><code>$ instructor usage list -n 3\n</code></pre> <p>This will output a table similar to:</p> <pre><code>                 Usage Summary by Date, Snapshot, and Cost\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Date       \u2503 Snapshot ID               \u2503 Total Requests \u2503 Total Cost ($) \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2023-09-04 \u2502 gpt-4-0613                \u2502             44 \u2502           0.68 \u2502\n\u2502 2023-09-04 \u2502 gpt-3.5-turbo-16k-0613    \u2502            195 \u2502           0.84 \u2502\n\u2502 2023-09-04 \u2502 text-embedding-ada-002-v2 \u2502            276 \u2502           0.00 \u2502\n\u2502 2023-09-04 \u2502 gpt-4-32k-0613            \u2502            328 \u2502          49.45 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/usage/#list-usage-for-today","title":"List Usage for Today","text":"<p>To display the API usage for today, simply run:</p> <pre><code>$ instructor usage list\n</code></pre>"},{"location":"cli/usage/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"examples/","title":"Function Calls by Example","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ul> <li> <p>Classifying Text: Single and multi-label classification using enums.</p> </li> <li> <p>Self-Assessment via Validators: Implement AI self-assessment with <code>llm_validator</code>.</p> </li> <li> <p>Citations via Regex: Retrieve exact citations using regular expressions and smart prompting.</p> </li> <li> <p>Extracting Search Queries: Segment search queries through function calling and multi-task definitions.</p> </li> <li> <p>Generating Knowledge Graphs: Generate knowledge graphs from a question</p> </li> <li> <p>Query Decomposition: Decompose complex queries into subqueries in a single request.</p> </li> <li> <p>Entity Extraction and Resolution: Extract and resolve entities from a document.</p> </li> <li> <p>Working with Recursive Schemas: Implement and understand recursive schemas.</p> </li> <li> <p>Table Extraction from Text: Extract tables, potentially multiple, automatically from textual data.</p> </li> <li> <p>Multi-File Code Generation: Generate multi-file programs with contents and paths. </p> </li> <li> <p>PII Data Sanitization: Extract and sanitize Personally Identifiable Information (PII) from documents.</p> </li> <li> <p>Action Item and Dependency Mapping: Generate action items and their dependencies from transcripts.</p> </li> </ul> <p>Happy exploring!</p>"},{"location":"examples/action_items/","title":"Example: Extracting Action Items from Meeting Transcripts","text":"<p>In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting.</p> <p>Motivation</p> <p>In the corporate world, a considerable amount of time is spent in meetings, and action items are often the actionable output of these discussions. Automating the extraction of action items can be a time-saver and ensures that nothing crucial is missed.</p>"},{"location":"examples/action_items/#defining-the-structures","title":"Defining the Structures","text":"<p>We'll model a meeting transcript as a collection of <code>Ticket</code> objects, each representing an action item. Every <code>Ticket</code> can have multiple <code>Subtask</code> objects, representing smaller, manageable pieces of the main task.</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n    id: int\n    name: str\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n\nclass ActionItems(BaseModel):\n    \"\"\"Correctly resolved set of action items from the given transcript\"\"\"\n    items: List[Ticket]\n</code></pre>"},{"location":"examples/action_items/#extracting-action-items","title":"Extracting Action Items","text":"<p>To extract action items from a meeting transcript, we use the <code>generate</code> function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as <code>ActionItems</code>.</p> <pre><code>import openai\n\ndef generate(data: str) -&gt; ActionItems:\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=ActionItems,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/action_items/#evaluation-and-testing","title":"Evaluation and Testing","text":"<p>To test the <code>generate</code> function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items.</p> <pre><code>prediction = generate(\n\"\"\"\nAlice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n\nBob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n\nAlice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n\nCarol: I can help with the front-end part of the authentication system.\n\nBob: Great, Carol. I'll handle the back-end optimization then.\n\nAlice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n\nCarol: Is the new billing system already in place?\n\nAlice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n\nBob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n\nAlice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n\nCarol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n\nAlice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\"\n)\n</code></pre>"},{"location":"examples/action_items/#visualizing-the-tasks","title":"Visualizing the tasks","text":"<p>In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array.</p> <p></p> <pre><code>{\n  \"items\": [\n    {\n      \"id\": 1,\n      \"name\": \"Improve Authentication System\",\n      \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n      \"priority\": \"High\",\n      \"assignees\": [\n        \"Bob\",\n        \"Carol\"\n      ],\n      \"subtasks\": [\n        {\n          \"id\": 2,\n          \"name\": \"Front-end Revamp\"\n        },\n        {\n          \"id\": 3,\n          \"name\": \"Back-end Optimization\"\n        }\n      ],\n      \"dependencies\": []\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Integrate Authentication System with Billing System\",\n      \"description\": \"Integrate the improved authentication system with the new billing system\",\n      \"priority\": \"Medium\",\n      \"assignees\": [\n        \"Bob\"\n      ],\n      \"subtasks\": [],\n      \"dependencies\": [\n        1\n      ]\n    },\n    {\n      \"id\": 5,\n      \"name\": \"Update User Documentation\",\n      \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n      \"priority\": \"Low\",\n      \"assignees\": [\n        \"Carol\"\n      ],\n      \"subtasks\": [],\n      \"dependencies\": [\n        2\n      ]\n    }\n  ]\n}\n</code></pre> <p>In this example, the <code>generate</code> function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting.</p> <p>By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective.</p>"},{"location":"examples/autodataframe/","title":"Example: Converting Text into Dataframes","text":"<p>In this example, we'll demonstrate how to convert a text into dataframes using OpenAI Function Call. We will define the necessary data structures using Pydantic and show how to convert the text into dataframes.</p> <p>Motivation</p> <p>Often times when we parse data we have an opportunity to extract structured data, what if we could extract an arbitrary number of tables with arbitrary schemas? By pulling out dataframes we could write tables or .csv files and attach them to our retrieved data.</p>"},{"location":"examples/autodataframe/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>Let's start by defining the data structures required for this task: <code>RowData</code>, <code>Dataframe</code>, and <code>Database</code>.</p> <pre><code>from instructor import OpenAISchema\nfrom pydantic import Field\nfrom typing import List, Any\n\n\nclass RowData(OpenAISchema):\n    row: List[Any] = Field(..., description=\"The values for each row\")\n    citation: str = Field(\n        ..., description=\"The citation for this row from the original source data\"\n    )\n\n\nclass Dataframe(OpenAISchema):\n    \"\"\"\n    Class representing a dataframe. This class is used to convert\n    data into a frame that can be used by pandas.\n    \"\"\"\n\n    name: str = Field(..., description=\"The name of the dataframe\")\n    data: List[RowData] = Field(\n        ...,\n        description=\"Correct rows of data aligned to column names, Nones are allowed\",\n    )\n    columns: List[str] = Field(\n        ...,\n        description=\"Column names relevant from source data, should be in snake_case\",\n    )\n\n    def to_pandas(self):\n        import pandas as pd\n\n        columns = self.columns + [\"citation\"]\n        data = [row.row + [row.citation] for row in self.data]\n\n        return pd.DataFrame(data=data, columns=columns)\n\n\nclass Database(OpenAISchema):\n    \"\"\"\n    A set of correct named and defined tables as dataframes\n    \"\"\"\n\n    tables: List[Dataframe] = Field(\n        ...,\n        description=\"List of tables in the database\",\n    )\n</code></pre> <p>The <code>RowData</code> class represents a single row of data in the dataframe. It contains a <code>row</code> attribute for the values in each row and a <code>citation</code> attribute for the citation from the original source data.</p> <p>The <code>Dataframe</code> class represents a dataframe and consists of a <code>name</code> attribute, a list of <code>RowData</code> objects in the <code>data</code> attribute, and a list of column names in the <code>columns</code> attribute. It also provides a <code>to_pandas</code> method to convert the dataframe into a Pandas DataFrame.</p> <p>The <code>Database</code> class represents a set of tables in a database. It contains a list of <code>Dataframe</code> objects in the <code>tables</code> attribute.</p>"},{"location":"examples/autodataframe/#using-the-prompt-pipeline","title":"Using the Prompt Pipeline","text":"<p>To convert a text into dataframes, we'll use the Prompt Pipeline in OpenAI Function Call. We can define a function <code>dataframe</code> that takes a text as input and returns a <code>Database</code> object.</p> <pre><code>import openai\n\ndef dataframe(data: str) -&gt; Database:\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-4-0613\",\n        temperature=0.1,\n        functions=[Database.openai_schema],\n        function_call={\"name\": Database.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Map this data into a dataframe a\n                nd correctly define the correct columns and rows\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"{data}\",\n            },\n        ],\n        max_tokens=1000,\n    )\n    return Database.from_response(completion)\n</code></pre> <p>The <code>dataframe</code> function takes a string <code>data</code> as input and creates a completion using the Prompt Pipeline. It prompts the model to map the data into a dataframe and define the correct columns and rows. The resulting completion is then converted into a <code>Database</code> object.</p>"},{"location":"examples/autodataframe/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate the example by converting a text into dataframes using the <code>dataframe</code> function and print the resulting dataframes.</p> <pre><code>dfs = dataframe(\"\"\"My name is John and I am 25 years old. I live in \nNew York and I like to play basketball. His name is \nMike and he is 30 years old. He lives in San Francisco \nand he likes to play baseball. Sarah is 20 years old \nand she lives in Los Angeles. She likes to play tennis.\nHer name is Mary and she is 35 years old. \nShe lives in Chicago.\n\nOn one team 'Tigers' the captain is John and there are 12 players.\nOn the other team 'Lions' the captain is Mike and there are 10 players.\n\"\"\")\n\nfor df in dfs.tables:\n    print(df.name)\n    print(df.to_pandas())\n</code></pre> <p>The output will be:</p> <pre><code>People\nName  Age           City Favorite Sport\n0   John   25       New York     Basketball\n1   Mike   30  San Francisco       Baseball\n2  Sarah   20    Los Angeles         Tennis\n3   Mary   35        Chicago           None\n\nTeams\nTeam Name Captain  Number of Players\n0    Tigers    John                 12\n1     Lions    Mike                 10\n</code></pre>"},{"location":"examples/classification/","title":"Example: Text Classification using OpenAI and Pydantic","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using the OpenAI API, Python's <code>enum</code> module, and Pydantic models.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures.</p>"},{"location":"examples/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a Pydantic model for the output.</p> <pre><code>import enum\nfrom pydantic import BaseModel\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n    class_label: Labels\n</code></pre>"},{"location":"examples/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>import openai\nimport instructor\n\n# Patch the OpenAI API to use the `ChatCompletion`\n# endpoint with `response_model` enabled.\ninstructor.patch() \n\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code># Test single-label classification\nprediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\nassert prediction.class_label == Labels.SPAM\n</code></pre>"},{"location":"examples/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code># Define Enum class for multiple labels\nclass MultiLabels(str, enum.Enum):\n    TECH_ISSUE = \"tech_issue\"\n    BILLING = \"billing\"\n    GENERAL_QUERY = \"general_query\"\n\n# Define the multi-class prediction model\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n    \"\"\"\n    class_labels: List[MultiLabels]\n</code></pre>"},{"location":"examples/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> is responsible for multi-label classification.</p> <pre><code>def multi_classify(data: str) -&gt; MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code># Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert MultiLabels.TECH_ISSUE in prediction.class_labels\nassert MultiLabels.BILLING in prediction.class_labels\n</code></pre>"},{"location":"examples/entity_resolution/","title":"Entity Resolution and Visualization for Legal Documents","text":"<p>In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents.</p> <p>Motivation</p> <p>Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms.</p>"},{"location":"examples/entity_resolution/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>The <code>Entity</code> and <code>Property</code> classes model extracted entities and their attributes. <code>DocumentExtraction</code> encapsulates a list of these entities.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be its seperate object with a body and a list of sources\",\n    )\n</code></pre>"},{"location":"examples/entity_resolution/#entity-extraction-and-resolution","title":"Entity Extraction and Resolution","text":"<p>The <code>ask_ai</code> function utilizes OpenAI's API to extract and resolve entities from the input content.</p> <pre><code>import openai\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\ninstructor.patch()\n\ndef ask_ai(content) -&gt; DocumentExtraction:\n    return openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/entity_resolution/#graph-visualization","title":"Graph Visualization","text":"<p><code>generate_graph</code> takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies.</p> <pre><code>from graphviz import Digraph\n\ndef generate_html_label(entity: Entity) -&gt; str:\n    rows = [f\"&lt;tr&gt;&lt;td&gt;{prop.key}&lt;/td&gt;&lt;td&gt;{prop.resolved_absolute_value}&lt;/td&gt;&lt;/tr&gt;\" for prop in entity.properties]\n    table_rows = \"\".join(rows)\n    return f\"&lt;table border='0' cellborder='1' cellspacing='0'&gt;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;b&gt;{entity.entity_title}&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;{table_rows}&lt;/table&gt;&gt;\"\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n</code></pre>"},{"location":"examples/entity_resolution/#execution","title":"Execution","text":"<p>Finally, execute the code to visualize the entity graph for the sample legal contract.</p> <pre><code>content = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n</code></pre> <p>This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\". </p> <p></p>"},{"location":"examples/exact_citations/","title":"Example: Answering Questions with Validated Citations","text":"<p>For the full code example check out examples/citation_fuzzy_match.py</p>"},{"location":"examples/exact_citations/#overview","title":"Overview","text":"<p>This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, <code>Fact</code> and <code>QuestionAnswer</code>, are defined to encapsulate the information of individual facts and the entire answer, respectively.</p>"},{"location":"examples/exact_citations/#data-structures","title":"Data Structures","text":""},{"location":"examples/exact_citations/#the-fact-class","title":"The <code>Fact</code> Class","text":"<p>The <code>Fact</code> class encapsulates a single statement or fact. It contains two fields:</p> <ul> <li><code>fact</code>: A string representing the body of the fact or statement.</li> <li><code>substring_quote</code>: A list of strings. Each string is a direct quote from the context that supports the <code>fact</code>.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method validates the sources (<code>substring_quote</code>) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list.</p> <pre><code>from pydantic import Field, BaseModel, model_validator, FieldValidationInfo\nfrom typing import List\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: FieldValidationInfo) -&gt; \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n</code></pre>"},{"location":"examples/exact_citations/#the-questionanswer-class","title":"The <code>QuestionAnswer</code> Class","text":"<p>This class encapsulates the question and its corresponding answer. It contains two fields:</p> <ul> <li><code>question</code>: The question asked.</li> <li><code>answer</code>: A list of <code>Fact</code> objects that make up the answer.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources_1","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method checks that each <code>Fact</code> object in the <code>answer</code> list has at least one valid source. If a <code>Fact</code> object has no valid sources, it is removed from the <code>answer</code> list.</p> <pre><code>class QuestionAnswer(instructor.OpenAISchema):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -&gt; \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) &gt; 0]\n        return self\n</code></pre>"},{"location":"examples/exact_citations/#function-to-ask-ai-a-question","title":"Function to Ask AI a Question","text":""},{"location":"examples/exact_citations/#the-ask_ai-function","title":"The <code>ask_ai</code> Function","text":"<p>This function takes a string <code>question</code> and a string <code>context</code> and returns a <code>QuestionAnswer</code> object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes.</p> <p>To understand the validation context work from pydantic check out pydantic's docs</p> <pre><code>def ask_ai(question: str, context: str) -&gt; QuestionAnswer:\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        functions=[QuestionAnswer.openai_schema],\n        function_call={\"name\": QuestionAnswer.openai_schema[\"name\"]},\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"},\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n        ],\n    )\n    return QuestionAnswer.from_response(\n        completion, validation_context={\"text_chunk\": context}\n    )\n</code></pre>"},{"location":"examples/exact_citations/#example","title":"Example","text":"<p>dd Here's an example of using these classes and functions to ask a question and validate the answer.</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics. \nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n</code></pre> <p>The output would be a <code>QuestionAnswer</code> object containing validated facts and their sources.</p> <pre><code>{\n  \"question\": \"where did he go to school?\",\n  \"answer\": [\n    {\n      \"statement\": \"Jason Liu went to an arts highschool.\",\n      \"substring_phrase\": [\n        \"arts highschool\"\n      ]\n    },\n    {\n      \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\",\n      \"substring_phrase\": [\n        \"university\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>This ensures that every piece of information in the answer has been validated against the context.</p>"},{"location":"examples/gpt-engineer/","title":"Example: Creating Multiple Files Program","text":"<p>This example shows how to create a multiple files program based on specifications by utilizing the OpenAI Function Call. We will define the necessary data structures using Pydantic and demonstrate how to convert a specification (prompt) into multiple files.</p> <p>Motivation</p> <p>Creating multiple file programs based on specifications is a challenging and rewarding skill that can help you build complex and scalable applications.  With OpenAI Function Call, you can leverage the power of language models to generate an entire codebase and code snippets that match your specifications.</p>"},{"location":"examples/gpt-engineer/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>Let's start by defining the data structure of <code>File</code> and <code>Program</code>.</p> <pre><code>from typing import List\nfrom pydantic import Field\nfrom instructor import OpenAISchema\n\n\nclass File(OpenAISchema):\n    \"\"\"\n    Correctly named file with contents.\n    \"\"\"\n\n    file_name: str = Field(\n        ..., description=\"The name of the file including the extension\"\n    )\n    body: str = Field(..., description=\"Correct contents of a file\")\n\n    def save(self):\n        with open(self.file_name, \"w\") as f:\n            f.write(self.body)\n\n\nclass Program(OpenAISchema):\n    \"\"\"\n    Set of files that represent a complete and correct program\n    \"\"\"\n\n    files: List[File] = Field(..., description=\"List of files\")\n</code></pre> <p>The <code>File</code> class represents a single file or script, and it contains a <code>name</code> attribute and <code>body</code> for the text content of the file.  Notice that we added the <code>save</code> method to the <code>File</code> class. This method is used to writes the body of the file to disk using the name as path.</p> <p>The <code>Program</code> class represents a collection of files that form a complete and correct program.  It contains a list of <code>File</code> objects in the <code>files</code> attribute.</p>"},{"location":"examples/gpt-engineer/#calling-completions","title":"Calling Completions","text":"<p>To create the files, we will use the base <code>openai</code> API.  We can define a function that takes in a string and returns a <code>Program</code> object.</p> <pre><code>import openai\n\ndef develop(data: str) -&gt; Program:\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0.1,\n        functions=[Program.openai_schema],\n        function_call={\"name\": Program.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include __init__.py files and write correct python code with correct imports.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": data,\n            },\n        ],\n        max_tokens=1000,\n    )\n    return Program.from_response(completion)\n</code></pre>"},{"location":"examples/gpt-engineer/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate the example by specifying the program to create and print the resulting files.</p> <pre><code>program = develop(\n        \"\"\"\n        Create a fastapi app with a readme.md file and a main.py file with \n        some basic math functions. the datamodels should use pydantic and \n        the main.py should use fastapi. the readme.md should have a title \n        and a description. The readme should contain some helpful infromation \n        and a curl example\"\"\"\n    )\n\nfor file in program.files:\n    print(file.file_name)\n    print(\"-\")\n    print(file.body)\n    print(\"\\n\\n\\n\")\n</code></pre> <p>The output will be: <pre><code># readme.md\n-\n    # FastAPI App\n\n    This is a FastAPI app that provides some basic math functions.\n\n    ## Usage\n\n    To use this app, follow the instructions below:\n\n    1. Install the required dependencies by running `pip install -r requirements.txt`.\n    2. Start the app by running `uvicorn main:app --reload`.\n    3. Open your browser and navigate to `http://localhost:8000/docs` to access the Swagger UI documentation.\n\n    ## Example\n\n    You can use the following curl command to test the `/add` endpoint:\n\n    ```bash\n    $ curl -X POST -H \"Content-Type: application/json\" -d '{\"a\": 2, \"b\": 3}' http://localhost:8000/add\n    ```\n</code></pre> <pre><code># main.py\n-\n    from fastapi import FastAPI\n    from pydantic import BaseModel\n\n    app = FastAPI()\n\n\n    class Numbers(BaseModel):\n        a: int\n        b: int\n\n\n    @app.post('/add')\n    def add_numbers(numbers: Numbers):\n        return {'result': numbers.a + numbers.b}\n\n\n    @app.post('/subtract')\n    def subtract_numbers(numbers: Numbers):\n        return {'result': numbers.a - numbers.b}\n\n\n    @app.post('/multiply')\n    def multiply_numbers(numbers: Numbers):\n        return {'result': numbers.a * numbers.b}\n\n\n    @app.post('/divide')\n    def divide_numbers(numbers: Numbers):\n        if numbers.b == 0:\n            return {'error': 'Cannot divide by zero'}\n        return {'result': numbers.a / numbers.b}\n</code></pre> <pre><code># requirements.txt\n-\n    fastapi\n    uvicorn\n    pydantic\n</code></pre></p>"},{"location":"examples/gpt-engineer/#add-refactoring-capabilities","title":"Add Refactoring Capabilities","text":"<p>This second part of the example shows how OpenAI API can be used to update the multiples files previously created, based on new specifications.</p> <p>In order to do that, we'll rely on the standard unidiff format.</p> <p>This will be our definition for a change in our code base:</p> <pre><code>from pydantic import Field\nfrom instructor import OpenAISchema\n\nclass Diff(OpenAISchema):\n    \"\"\"\n    Changes that must be correctly made in a program's code repository defined as a\n    complete diff (Unified Format) file which will be used to `patch` the repository.\n\n    Example:\n      --- /path/to/original timestamp\n      +++ /path/to/new  timestamp\n      @@ -1,3 +1,9 @@\n      +This is an important\n      +notice! It should\n      +therefore be located at\n      +the beginning of this\n      +document!\n      +\n       This part of the\n       document has stayed the\n       same from version to\n      @@ -8,13 +14,8 @@\n       compress the size of the\n       changes.\n      -This paragraph contains\n      -text that is outdated.\n      -It will be deleted in the\n      -near future.\n      -\n       It is important to spell\n      -check this dokument. On\n      +check this document. On\n       the other hand, a\n       misspelled word isn't\n       the end of the world.\n      @@ -22,3 +23,7 @@\n       this paragraph needs to\n       be changed. Things can\n       be added after it.\n      +\n      +This paragraph contains\n      +important new additions\n      +to this document.\n    \"\"\"\n\n    diff: str = Field(\n        ...,\n        description=(\n            \"Changes in a code repository correctly represented in 'diff' format, \"\n            \"correctly escaped so it could be used in a JSON\"\n        ),\n    )\n</code></pre> <p>The <code>diff</code> class represents a diff file, with a set of changes that can be applied to our program using a tool like patch or Git.</p>"},{"location":"examples/gpt-engineer/#calling-refactor-completions","title":"Calling Refactor Completions","text":"<p>We'll define a function that will pass the program and the new specifications to the OpenAI API:</p> <pre><code>import openai\nfrom generate import Program\n\ndef refactor(new_requirements: str, program: Program) -&gt; Diff:\n    program_description = \"\\n\".join(\n        [f\"{code.file_name}\\n[[[\\n{code.body}\\n]]]\\n\" for code in program.files]\n    )\n    completion = openai.ChatCompletion.create(\n        # model=\"gpt-3.5-turbo-0613\",\n        model=\"gpt-4\",\n        temperature=0,\n        functions=[Diff.openai_schema],\n        function_call={\"name\": Diff.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class programming AI capable of refactor \"\n                \"existing python repositories. You will name files correct, include \"\n                \"__init__.py files and write correct python code, with correct imports. \"\n                \"You'll deliver your changes in valid 'diff' format so that they could \"\n                \"be applied using the 'patch' command. \"\n                \"Make sure you put the correct line numbers, \"\n                \"and that all lines that must be changed are correctly marked.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": new_requirements,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": program_description,\n            },\n        ],\n        max_tokens=1000,\n    )\n    return Diff.from_response(completion)\n</code></pre> <p>Notice we're using here the version <code>gpt-4</code> of the model, which is more powerful but, also, more expensive.</p>"},{"location":"examples/gpt-engineer/#creating-an-example-refactoring","title":"Creating an Example Refactoring","text":"<p>To tests these refactoring, we'll use the <code>program</code> object, generated in the first part of this example.</p> <pre><code>changes = refactor(\n    new_requirements=\"Refactor this code to use flask instead.\",\n    program=program,\n)\nprint(changes.diff)\n</code></pre> <p>The output will be this:</p> <pre><code>--- readme.md\n+++ readme.md\n@@ -1,9 +1,9 @@\n # FastAPI App\n\n-This is a FastAPI app that provides some basic math functions.\n+This is a Flask app that provides some basic math functions.\n\n ## Usage\n\n To use this app, follow the instructions below:\n\n 1. Install the required dependencies by running `pip install -r requirements.txt`.\n-2. Start the app by running `uvicorn main:app --reload`.\n+2. Start the app by running `flask run`.\n 3. Open your browser and navigate to `http://localhost:5000/docs` to access the Swagger UI documentation.\n\n ## Example\n\n To perform a basic math operation, you can use the following curl command:\n\n ```bash\n-curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": [2, 3]}' http://localhost:8000/calculate\n+curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": [2, 3]}' http://localhost:5000/calculate\n ```\n\n--- main.py\n+++ main.py\n@@ -1,29 +1,29 @@\n-from fastapi import FastAPI\n-from pydantic import BaseModel\n+from flask import Flask, request, jsonify\n\n-app = FastAPI()\n+app = Flask(__name__)\n\n\n-class Operation(BaseModel):\n-    operation: str\n-    operands: list\n+@app.route('/calculate', methods=['POST'])\n+def calculate():\n+    data = request.get_json()\n+    operation = data.get('operation')\n+    operands = data.get('operands')\n\n\n-@app.post('/calculate')\n-async def calculate(operation: Operation):\n-    if operation.operation == 'add':\n-        result = sum(operation.operands)\n-    elif operation.operation == 'subtract':\n-        result = operation.operands[0] - sum(operation.operands[1:])\n-    elif operation.operation == 'multiply':\n+    if operation == 'add':\n+        result = sum(operands)\n+    elif operation == 'subtract':\n+        result = operands[0] - sum(operands[1:])\n+    elif operation == 'multiply':\n         result = 1\n-        for operand in operation.operands:\n+        for operand in operands:\n             result *= operand\n-    elif operation.operation == 'divide':\n-        result = operation.operands[0]\n-        for operand in operation.operands[1:]:\n+    elif operation == 'divide':\n+        result = operands[0]\n+        for operand in operands[1:]:\n             result /= operand\n     else:\n         result = None\n-    return {'result': result}\n+    return jsonify({'result': result})\n\n--- requirements.txt\n+++ requirements.txt\n@@ -1,3 +1,2 @@\n-fastapi\n-uvicorn\n-pydantic\n+flask\n+flask-cors\n</code></pre>"},{"location":"examples/knowledge_graph/","title":"Visualizing Knowledge Graphs for Complex Topics","text":"<p>In this guide, you'll discover how to visualize a detailed knowledge graph for understanding complex topics, in this case, quantum mechanics. We leverage OpenAI's API and the Graphviz library to bring structure to intricate subjects.</p> <p>Motivation</p> <p>Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information.</p>"},{"location":"examples/knowledge_graph/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model a knowledge graph with <code>Node</code> and <code>Edge</code> objects. <code>Node</code> objects represent key concepts or entities, while <code>Edge</code> objects indicate the relationships between them.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n</code></pre>"},{"location":"examples/knowledge_graph/#generating-knowledge-graphs","title":"Generating Knowledge Graphs","text":"<p>The <code>generate_graph</code> function leverages OpenAI's API to generate a knowledge graph based on the input query.</p> <pre><code>import openai\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\ninstructor.patch()\n\ndef generate_graph(input) -&gt; KnowledgeGraph:\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )  # type: ignore\n</code></pre>"},{"location":"examples/knowledge_graph/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>The <code>visualize_knowledge_graph</code> function uses the Graphviz library to render the generated knowledge graph.</p> <pre><code>from graphviz import Digraph\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n</code></pre>"},{"location":"examples/knowledge_graph/#putting-it-all-together","title":"Putting It All Together","text":"<p>Execute the code to generate and visualize a knowledge graph for understanding quantum mechanics.</p> <pre><code>graph: KnowledgeGraph = generate_graph(\"Teach me about quantum mechanics\")\nvisualize_knowledge_graph(graph)\n</code></pre> <p></p> <p>This will produce a visual representation of the knowledge graph, stored as \"knowledge_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics.</p> <p>By leveraging automated knowledge graphs, you can dissect complex topics into digestible pieces, making the learning journey less daunting and more effective.</p>"},{"location":"examples/pii/","title":"PII Data Extraction and Scrubbing","text":""},{"location":"examples/pii/#overview","title":"Overview","text":"<p>This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.</p>"},{"location":"examples/pii/#defining-the-structures","title":"Defining the Structures","text":"<p>First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -&gt; str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        &lt;{data_type}_{i}&gt;\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"&lt;{data.data_type}_{i}&gt;\")\n        return content\n</code></pre>"},{"location":"examples/pii/#extracting-pii-data","title":"Extracting PII Data","text":"<p>The OpenAI API is utilized to extract PII information from a given document.</p> <pre><code>import openai\nimport instructor\n\ninstructor.patch()\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data: PIIDataExtraction = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ]\n)  # type: ignore\n\nprint(\"Extracted PII Data:\")\nprint(pii_data.json(indent=2))\n</code></pre>"},{"location":"examples/pii/#output-of-extracted-pii-data","title":"Output of Extracted PII Data","text":"<pre><code>{\n  \"private_data\": [\n    {\n      \"index\": 0,\n      \"data_type\": \"date\",\n      \"pii_value\": \"01/02/1980\"\n    },\n    {\n      \"index\": 1,\n      \"data_type\": \"ssn\",\n      \"pii_value\": \"123-45-6789\"\n    },\n    {\n      \"index\": 2,\n      \"data_type\": \"email\",\n      \"pii_value\": \"john.doe@email.com\"\n    },\n    {\n      \"index\": 3,\n      \"data_type\": \"phone\",\n      \"pii_value\": \"555-123-4567\"\n    },\n    {\n      \"index\": 4,\n      \"data_type\": \"address\",\n      \"pii_value\": \"123 Main St, Springfield, IL, 62704\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/pii/#scrubbing-pii-data","title":"Scrubbing PII Data","text":"<p>After extracting the PII data, the <code>scrub_data</code> method is used to sanitize the document.</p> <pre><code>print(\"Scrubbed Document:\")\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n</code></pre>"},{"location":"examples/pii/#output-of-scrubbed-document","title":"Output of Scrubbed Document","text":"<pre><code># Fake Document with PII for Testing PII Scrubbing Model\n\n## Personal Story\n\nJohn Doe was born on &lt;date_0&gt;. His social security number is &lt;ssn_1&gt;. He has been using the email address &lt;email_2&gt; for years, and he can always be reached at &lt;phone_3&gt;.\n\n## Residence\n\nJohn currently resides at &lt;address_4&gt;. He's been living there for about 5 years now.\n</code></pre>"},{"location":"examples/planning-tasks/","title":"Example: Planning and Executing a Query Plan","text":"<p>This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question.</p> <p>Motivation</p> <p>The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively.</p> <p>Use Cases:</p> <ul> <li>Complex question answering</li> <li>Iterative information gathering</li> <li>Workflow automation</li> <li>Process optimization</li> </ul> <p>With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements.</p>"},{"location":"examples/planning-tasks/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's define the necessary Pydantic models to represent the query plan and the queries.</p> <pre><code>import enum\nfrom typing import List\n\nfrom pydantic import Field\nfrom instructor import OpenAISchema\n\n\nclass QueryType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n\n    SINGLE_QUESTION = \"SINGLE\"\n    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n\n\nclass Query(OpenAISchema):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependancies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: QueryType = Field(\n        default=QueryType.SINGLE_QUESTION,\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(OpenAISchema):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -&gt; List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n</code></pre> <p>Graph Generation</p> <p>Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas</p>"},{"location":"examples/planning-tasks/#planning-a-query-plan","title":"Planning a Query Plan","text":"<p>Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API.</p> <pre><code>import asyncio\n\nimport openai\n\ndef query_planner(question: str) -&gt; QueryPlan:\n    PLANNING_MODEL = \"gpt-4-0613\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    completion = openai.ChatCompletion.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        functions=[QueryPlan.openai_schema],\n        function_call={\"name\": QueryPlan.openai_schema[\"name\"]},\n        messages=messages,\n        max_tokens=1000,\n    )\n    root = QueryPlan.from_response(completion)\n    return root\n</code></pre> <pre><code>plan = query_planner(\n    \"What is the difference in populations of Canada and the Jason's home country?\"\n)\nplan.dict()\n</code></pre> <p>No RAG</p> <p>While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrival and calls openai for retrival augmented generation. That step would also make use of function calls but goes beyond the scope of this example.</p> <pre><code>{'query_graph': [{'dependancies': [],\n                    'id': 1,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': \"Identify Jason's home country\"},\n                    {'dependancies': [],\n                    'id': 2,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': 'Find the population of Canada'},\n                    {'dependancies': [1],\n                    'id': 3,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': \"Find the population of Jason's home country\"},\n                    {'dependancies': [2, 3],\n                    'id': 4,\n                    'node_type': &lt;QueryType.SINGLE_QUESTION: 'SINGLE'&gt;,\n                    'question': 'Calculate the difference in populations between '\n                                \"Canada and Jason's home country\"}]} \n</code></pre> <p>In the above code, we define a <code>query_planner</code> function that takes a question as input and generates a query plan using the OpenAI API.</p>"},{"location":"examples/planning-tasks/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to use the OpenAI Function Call <code>ChatCompletion</code> model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function. </p> <p>If you want to see multiple versions of this style of code, please visit:</p> <ol> <li>query planning example</li> <li>task planning with topo sort</li> </ol> <p>Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows.</p>"},{"location":"examples/recursive/","title":"Example: Parsing a Directory Tree","text":"<p>In this example, we will demonstrate how define and use a recursive class definition to convert a string representing a directory tree into a filesystem structure using OpenAI's function call api. We will define the necessary structures using Pydantic, create a function to parse the tree, and provide an example of how to use it.</p>"},{"location":"examples/recursive/#defining-the-structures","title":"Defining the Structures","text":"<p>We will use Pydantic to define the necessary data structures representing the directory tree and its nodes. We have two classes, <code>Node</code> and <code>DirectoryTree</code>, which are used to model individual nodes and the entire directory tree, respectively.</p> <p>Flat is better than nested</p> <p>While it's easier to model things as nested, returning flat items with dependencies tends to yield better results. For a flat example, check out planning tasks where we model a query plan as a dag.</p> <pre><code>import enum\nfrom typing import List\nfrom pydantic import Field\nfrom instructor import OpenAISchema\n\nclass NodeType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of nodes in a filesystem.\"\"\"\n    FILE = \"file\"\n    FOLDER = \"folder\"\n\nclass Node(OpenAISchema):\n    \"\"\"\n    Class representing a single node in a filesystem. Can be either a file or a folder.\n    Note that a file cannot have children, but a folder can.\n\n    Args:\n        name (str): The name of the node.\n        children (List[Node]): The list of child nodes (if any).\n        node_type (NodeType): The type of the node, either a file or a folder.\n\n    Methods:\n        print_paths: Prints the path of the node and its children.\n    \"\"\"\n    name: str = Field(..., description=\"Name of the folder\")\n    children: List[\"Node\"] = Field(\n        default_factory=list,\n        description=\"List of children nodes, only applicable for folders, files cannot have children\",\n    )\n    node_type: NodeType = Field(\n        default=NodeType.FILE,\n        description=\"Either a file or folder, use the name to determine which it could be\",\n    )\n\n    def print_paths(self, parent_path=\"\"):\n        \"\"\"Prints the path of the node and its children.\"\"\"\n        if self.node_type == NodeType.FOLDER:\n            path = f\"{parent_path}/{self.name}\" if parent_path != \"\" else self.name\n            print(path, self.node_type)\n            if self.children is not None:\n                for child in self.children:\n                    child.print_paths(path)\n        else:\n            print(f\"{parent_path}/{self.name}\", self.node_type)\n\nclass DirectoryTree(OpenAISchema):\n    \"\"\"\n    Container class representing a directory tree.\n\n    Args:\n        root (Node): The root node of the tree.\n\n    Methods:\n        print_paths: Prints the paths of the root node and its children.\n    \"\"\"\n    root: Node = Field(..., description=\"Root folder of the directory tree\")\n\n    def print_paths(self):\n        \"\"\"Prints the paths of the root node and its children.\"\"\"\n        self.root.print_paths()\n\nNode.update_forward_refs()\nDirectoryTree.update_forward_refs()\n</code></pre> <p>The <code>Node</code> class represents a single node in the directory tree. It has a name, a list of children nodes (applicable only to folders), and a node type (either a file or a folder). The <code>print_paths</code> method can be used to print the path of the node and its children.</p> <p>The <code>DirectoryTree</code> class represents the entire directory tree. It has a single attribute, <code>root</code>, which is the root node of the tree. The <code>print_paths</code> method can be used to print the paths of the root node and its children.</p>"},{"location":"examples/recursive/#parsing-the-tree","title":"Parsing the Tree","text":"<p>We define a function <code>parse_tree_to_filesystem</code> to convert a string representing a directory tree into a filesystem structure using OpenAI.</p> <pre><code>import openai\n\ndef parse_tree_to_filesystem(data: str) -&gt; DirectoryTree:\n    \"\"\"\n    Convert a string representing a directory tree into a filesystem structure\n    using OpenAI's GPT-3 model.\n\n    Args:\n        data (str): The string to convert into a filesystem.\n\n    Returns:\n        DirectoryTree: The directory tree representing the filesystem.\n    \"\"\"\n\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0.2,\n        functions=[DirectoryTree.openai_schema],\n        function_call={\"name\": DirectoryTree.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a perfect file system parsing algorithm. You are given a string representing a directory tree. You must return the correct filesystem structure.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below:\\n{data} and return the correctly labeled filesystem\",\n            },\n        ],\n        max_tokens=1000,\n    )\n    root = DirectoryTree.from_response(completion)\n    return root\n</code></pre> <p>The <code>parse_tree_to_filesystem</code> function takes a string <code>data</code> representing the directory tree and returns a <code>DirectoryTree</code> object representing the filesystem structure. It uses the OpenAI Chat API to complete the prompt and extract the directory tree.</p>"},{"location":"examples/recursive/#example-usage","title":"Example Usage","text":"<p>Let's demonstrate how to use the <code>parse_tree_to_filesystem</code></p> <p>function with an example:</p> <pre><code>root = parse_tree_to_filesystem(\n    \"\"\"\n    root\n    \u251c\u2500\u2500 folder1\n    \u2502   \u251c\u2500\u2500 file1.txt\n    \u2502   \u2514\u2500\u2500 file2.txt\n    \u2514\u2500\u2500 folder2\n        \u251c\u2500\u2500 file3.txt\n        \u2514\u2500\u2500 subfolder1\n            \u2514\u2500\u2500 file4.txt\n    \"\"\"\n)\nroot.print_paths()\n</code></pre> <p>In this example, we call <code>parse_tree_to_filesystem</code> with a string representing a directory tree.</p> <p>After parsing the string into a <code>DirectoryTree</code> object, we call <code>root.print_paths()</code> to print the paths of the root node and its children. The output of this example will be:</p> <pre><code>root                               NodeType.FOLDER\nroot/folder1                       NodeType.FOLDER\nroot/folder1/file1.txt             NodeType.FILE\nroot/folder1/file2.txt             NodeType.FILE\nroot/folder2                       NodeType.FOLDER\nroot/folder2/file3.txt             NodeType.FILE\nroot/folder2/subfolder1            NodeType.FOLDER\nroot/folder2/subfolder1/file4.txt  NodeType.FILE\n</code></pre> <p>This demonstrates how to use OpenAI's GPT-3 model to parse a string representing a directory tree and obtain the correct filesystem structure.</p> <p>I hope this example helps you understand how to leverage OpenAI Function Call for parsing recursive trees. If you have any further questions, feel free to ask!</p>"},{"location":"examples/search/","title":"Example: Segmenting Search Queries","text":"<p>In this example, we will demonstrate how to leverage the <code>MultiTask</code> and <code>enum.Enum</code> features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with <code>asyncio</code>.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel.</p>"},{"location":"examples/search/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model the problem as breaking down a search request into a list of search queries. We will use an enum to represent different types of searches and take advantage of Python objects to add additional query logic.</p> <pre><code>import enum\nfrom pydantic import Field\nfrom instructor import OpenAISchema\n\nclass SearchType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of searches that can be performed.\"\"\"\n    VIDEO = \"video\"\n    EMAIL = \"email\"\n\nclass Search(OpenAISchema):\n    \"\"\"\n    Class representing a single search query.\n    \"\"\"\n    title: str = Field(..., description=\"Title of the request\")\n    query: str = Field(..., description=\"Query to search for relevant content\")\n    type: SearchType = Field(..., description=\"Type of search\")\n\n    async def execute(self):\n        print(f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\")\n</code></pre> <p>Notice that we have added the <code>execute</code> method to the <code>Search</code> class. This method can be used to route the search query based on the enum type. You can add logic specific to each search type in the <code>execute</code> method.</p> <p>Next, let's define a class to represent multiple search queries.</p> <pre><code>from typing import List\n\nclass MultiSearch(OpenAISchema):\n    \"Correctly segmented set of search results\"\n    tasks: List[Search]\n</code></pre> <p>The <code>MultiSearch</code> class has a single attribute, <code>tasks</code>, which is a list of <code>Search</code> objects.</p> <p>This pattern is so common that we've added a helper function <code>MultiTask</code> to makes this simpler </p> <pre><code>from instructor.dsl import MultiTask\n\nMultiSearch = MultiTask(Search)\n</code></pre>"},{"location":"examples/search/#calling-completions","title":"Calling Completions","text":"<p>To segment a search query, we will use the base openai api. We can define a function that takes a string and returns segmented search queries using the <code>MultiSearch</code> class.</p> <pre><code>import openai\n\ndef segment(data: str) -&gt; MultiSearch:\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0.1,\n        functions=[MultiSearch.openai_schema],\n        function_call={\"name\": MultiSearch.openai_schema[\"name\"]},\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",\n            },\n        ],\n        max_tokens=1000,\n    )\n\n    return MultiSearch.from_response(completion)\n</code></pre> <p>The <code>segment</code> function takes a string <code>data</code> and creates a completion. It prompts the model to segment the data into multiple search queries and returns the result as a <code>MultiSearch</code> object.</p>"},{"location":"examples/search/#evaluating-an-example","title":"Evaluating an Example","text":"<p>Let's evaluate an example by segmenting a search query and executing the segmented queries.</p> <pre><code>import asyncio\n\nqueries = segment(\"Please send me the video from last week about the investment case study and also documents about your GDPR policy?\")\n\nasync def execute_queries(queries: Multisearch):\n    await asyncio.gather(*[q.execute() for q in queries.tasks])\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(execute_queries())\nloop.close()\n</code></pre> <p>In this example, we use the <code>segment</code> function to segment the search query. We then use <code>asyncio</code> to asynchronously execute the queries using the <code>execute</code> method defined in the <code>Search</code> class.</p> <p>The output will be:</p> <pre><code>Searching for `Please send me the video from last week about the investment case study` with query `Please send me the video from last week about the investment case study` using `SearchType.VIDEO`\nSearching for `also documents about your GDPR policy?` with query `also documents about your GDPR policy?` using `SearchType.EMAIL`\n</code></pre>"},{"location":"examples/self_critique/","title":"Self-Correction with <code>llm_validator</code>","text":""},{"location":"examples/self_critique/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to use <code>llm_validator</code> for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages.</p>"},{"location":"examples/self_critique/#setup","title":"Setup","text":"<p>Import required modules and apply compatibility patches.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator, patch\nimport openai\n\npatch()\n</code></pre>"},{"location":"examples/self_critique/#defining-models","title":"Defining Models","text":"<p>Before building validation logic, define a basic Pydantic model named <code>QuestionAnswer</code>. We'll use this model to generate a response without validation to see the output.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n</code></pre>"},{"location":"examples/self_critique/#generating-a-response","title":"Generating a Response","text":"<p>Here we coerce the model to generate a response that is objectionable.</p> <pre><code>question = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#output-before-validation","title":"Output Before Validation","text":"<p>While it calls out the objectionable content, it doesn't provide any details on how to correct it.</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n</code></pre>"},{"location":"examples/self_critique/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>By adding a validator to the <code>answer</code> field, we can try to catch the issue and correct it. Lets integrate <code>llm_validator</code> into the model and see the error message. Its important to not that you can use all of pydantic's validators as you would normally as long as you raise a <code>ValidationError</code> with a helpful error message as it will be used as part of the self correction prompt.</p> <pre><code>class QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", allow_override=True)\n        ),\n    ]\n\ntry:\n    qa: QuestionAnswerNoEvil = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n</code></pre>"},{"location":"examples/self_critique/#output-after-validation","title":"Output After Validation","text":"<p>Now, we throw validation error that its objectionable and provide a helpful error message.</p> <pre><code>1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n</code></pre>"},{"location":"examples/self_critique/#retrying-with-corrections","title":"Retrying with Corrections","text":"<p>By adding the <code>max_retries</code> parameter, we can retry the request with corrections. and use the error message to correct the output.</p> <pre><code>qa: QuestionAnswerNoEvil = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=1,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#final-output","title":"Final Output","text":"<p>Now, we get a valid response that is not objectionable!</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n</code></pre>"},{"location":"tips/","title":"Prompt Engineering for Function Calling","text":"<p>The overarching theme of using instructor and pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use Pydantic's <code>Field</code> for clear field descriptions.</li> <li>Optionality: Use Python's <code>Optional</code> type for nullable fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"tips/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(..., \n        description=\"Think step by step to determine the correct title\")\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"tips/#utilize-optional-attributes","title":"Utilize Optional Attributes","text":"<p>Use Python's Optional type and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>from typing import Optional\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"tips/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in message.</p>"},{"location":"tips/#simplification-with-the-maybe-pattern","title":"Simplification with the Maybe Pattern","text":"<p>You can further simplify this using instructor to create the <code>Maybe</code> pattern dynamically from any <code>BaseModel</code>.</p> <pre><code>import instructor\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre> <p>This allows you to quickly create a Maybe type for any class, streamlining the process.</p>"},{"location":"tips/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\")\n</code></pre>"},{"location":"tips/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>class Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n    instructions: str = Field(..., description=\"Restate the instructions and rules to correctly determine the title.\")\n    title: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"tips/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>from typing import List\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(..., description=\"Extract any other properties that might be relevant.\")\n</code></pre>"},{"location":"tips/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>class Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\")\n</code></pre> <p>Using Tuples for Simple Types</p> <p>For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.</p> <pre><code>class UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\")\n</code></pre>"},{"location":"tips/#advanced-arbitrary-properties","title":"Advanced Arbitrary Properties","text":"<p>For multiple users, aim to use consistent key names when extracting properties.</p> <pre><code>class UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users. \n    Use consistent key names for properties across users.\n    \"\"\"\n    users: List[UserDetail]\n</code></pre> <p>This refined guide should offer a cleaner and more organized approach to structure engineering in Python.</p>"},{"location":"tips/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:</p> <pre><code>class UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(..., description=\"Correct and complete list of friend IDs, representing relationships between users.\")\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(..., description=\"Collection of users, correctly capturing the relationships among them.\")\n</code></pre>"},{"location":"tips/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work_time and leisure_time.</p> <pre><code>class TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(..., description=\"Time range during which the user is working.\")\n    leisure_time: TimeRange = Field(..., description=\"Time range reserved for leisure activities.\")\n</code></pre> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str = Field(..., description=\"Step by step reasoning to get the correct time range\")\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n</code></pre>"},{"location":"blog/archive/2023/","title":"2023","text":""}]}